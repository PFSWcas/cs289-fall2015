%PDF-1.2 %‚„œ”
 10 0 obj<</Length 11 0 R>>stream
BT234 708  TD0 0 0 rg /F0 14.25  Tf0.0526  Tc 0.1349  Tw (Kernel ridge regression) Tj144 0  TD 0  Tc 0.1875  Tw ( ) Tj-288 -14.25  TD /F1 12  Tf0  Tw ( ) Tj97.5 -14.25  TD -0.4218  Tc 0.0468  Tw (Isabelle Guyon ) Tj70.5 0  TD 0  Tc 0  Tw (\226) Tj6 0  TD ( ) Tj3 0  TD 0 0 1 rg -0.3071  Tc (Isabelle@clopinet.com) TjET267 677.25 103.5 0.75 re fBT370.5 679.5  TD0 0 0 rg -0.194  Tc 0.194  Tw (, June 2005) Tj53.25 0  TD 0  Tc 0  Tw ( ) TjET88.5 674.25 435 0.75 re fBT90 663.75  TD( ) Tj0 -13.5  TD -0.4177  Tc 0.36  Tw (The kernel ridge regression method \(see e.g. the \223The Elements of Statistical Learning\224 ) TjT* -0.4242  Tc 0.4242  Tw (by T. Hastie) Tj55.5 0  TD 0  Tc 0  Tw (  ) Tj6 0  TD -0.528  Tc 0.528  Tw (R. Tibshirani) Tj57 0  TD 0  Tc 0  Tw (  ) Tj6 0  TD -0.2679  Tc 0.2679  Tw (J. H. Friedman, S) Tj80.25 0  TD -0.2959  Tc 0.2959  Tw (pringer, 2001\) is a ) Tj86.25 0  TD /F0 12  Tf-0.1546  Tc 0.4046  Tw (regularized least square ) Tj-291 -14.25  TD -0.194  Tc 0  Tw (method) Tj36.75 0  TD /F1 12  Tf-0.4647  Tc 0.4647  Tw ( for classification and regression. The linear version is similar to ) Tj285 0  TD /F0 12  Tf-0.1035  Tc 0.8535  Tw (Fisher\222s ) Tj-321.75 -13.5  TD -0.445  Tc 0  Tw (discriminant) Tj60.75 0  TD /F1 12  Tf-0.4365  Tc 0.4365  Tw ( for classification. The non) Tj117.75 0  TD -0.246  Tc 0  Tw (-) Tj4.5 0  TD -0.5104  Tc 0.5104  Tw (linear version is ) Tj72 0  TD /F0 12  Tf-0.2559  Tc 0.2559  Tw (similar to an SVM) Tj91.5 0  TD /F1 12  Tf-0.2564  Tc 0.2564  Tw (, except that ) Tj-346.5 -14.25  TD -0.4656  Tc 0.4656  Tw (a different objective is being optimi) Tj156.75 0  TD -0.3489  Tc 0.3489  Tw (zed, which does not put emphasis on points close to ) Tj-156.75 -13.5  TD -0.4015  Tc 0.4015  Tw (the decision boundary. The solution depends on ALL the training examples \(not on a ) Tj0 -13.5  TD -0.3477  Tc 0.2942  Tw (subset of support vectors.\) Hence, the method is suitable only for datasets with few ) Tj0 -14.25  TD -0.4723  Tc 0.4723  Tw (training examples. For the linear ve) Tj156.75 0  TD -0.4788  Tc 0.4038  Tw (rsion, the kernel trick is useful if the number of ) Tj-156.75 -13.5  TD -0.3815  Tc 0.3815  Tw (features is large and the number of examples small. In the opposite case, one should not ) Tj0 -14.25  TD -0.3753  Tc 0.3284  Tw (use the kernel trick and work in direct space. This allows us to build ridge regression ) Tj0 -13.5  TD -0.4985  Tc 0.4985  Tw (linear classifiers for a sma) Tj114.75 0  TD -0.497  Tc 0.4137  Tw (ll number of features and many training examples. An ) Tj-114.75 -13.5  TD -0.4052  Tc 0.4052  Tw (advantage of kernel ridge regression is that there exist formulas to compute the leave) Tj378.75 0  TD -0.246  Tc 0  Tw (-) Tj-378.75 -14.25  TD -0.526  Tc (one) Tj16.5 0  TD -0.246  Tc (-) Tj4.5 0  TD -0.3326  Tc 0.3326  Tw (out mean) Tj41.25 0  TD -0.246  Tc 0  Tw (-) Tj4.5 0  TD -0.3885  Tc 0.3885  Tw (squared error and classification error rate using the results of a single ) Tj-66.75 -13.5  TD -0.5462  Tc 0.5462  Tw (training on the whole trainin) Tj123 0  TD -0.4674  Tc 0.4674  Tw (g set, i.e. without actually performing the leave) Tj207.75 0  TD -0.246  Tc 0  Tw (-) Tj4.5 0  TD -0.526  Tc (one) Tj16.5 0  TD -0.246  Tc (-) Tj4.5 0  TD -0.2715  Tc 0.2715  Tw (out. ) Tj-356.25 -14.25  TD -0.3133  Tc 0.3133  Tw (Hence, the hyper) Tj77.25 0  TD -0.246  Tc 0  Tw (-) Tj4.5 0  TD -0.3271  Tc 0.3271  Tw (parameters \(the ridge and the kernel parameters\) can be optimized ) Tj-81.75 -13.5  TD -0.5103  Tc 0.5103  Tw (efficiently. In addition, if an implementation with singular value decomposition is made, ) Tj0 -13.5  TD -0.3628  Tc 0.3628  Tw (one can compute with a s) Tj114.75 0  TD -0.4829  Tc 0.4829  Tw (ingle training the solutions corresponding to many values of the ) Tj-114.75 -14.25  TD -0.4417  Tc 0.4417  Tw (ridge. Combined with the leave) Tj140.25 0  TD -0.246  Tc 0  Tw (-) Tj4.5 0  TD -0.526  Tc (one) Tj16.5 0  TD -0.246  Tc (-) Tj4.5 0  TD -0.4577  Tc 0.3639  Tw (out formula, this renders the ridge optimization very ) Tj-165.75 -13.5  TD -0.5484  Tc 0  Tw (efficient.) Tj37.5 0  TD 0  Tc ( ) Tj-37.5 -14.25  TD ( ) Tj0 -13.5  TD -0.153  Tc (Pseudo) Tj33.75 0  TD -0.246  Tc (-) Tj4.5 0  TD -0.4596  Tc 0.4596  Tw (inverse and ridge) TjET90 372 114.75 0.75 re fBT204.75 374.25  TD0  Tc 0  Tw ( ) Tj-114.75 -13.5  TD ( ) Tj0 -14.25  TD -0.4233  Tc 0.4233  Tw (We present an implementation of kernel ridge regression using th) Tj290.25 0  TD -0.1534  Tc 0.1534  Tw (e pseudo) Tj41.25 0  TD -0.246  Tc 0  Tw (-) Tj4.5 0  TD -0.3312  Tc 0.3312  Tw (inverse. We ) Tj-336 -13.5  TD -0.3636  Tc 0.3636  Tw (first describe the linear case and then move to the non) Tj241.5 0  TD -0.246  Tc 0  Tw (-) Tj4.5 0  TD -0.3853  Tc 0.3853  Tw (linear case via the kernel trick.) Tj136.5 0  TD 0  Tc 0  Tw ( ) Tj-382.5 -14.25  TD ( ) Tj0 -13.5  TD -0.3464  Tc 0.3464  Tw (Let X be the data matrix of dimension \(p, n\), p patterns, n features. Let ) Tj322.5 0  TD /F0 12  Tf0  Tc 0  Tw (y) Tj6 0  TD /F1 12  Tf-0.2787  Tc 0.2787  Tw ( be the target ) Tj-328.5 -13.5  TD -0.3907  Tc 0.3225  Tw (matrix of dimension \(p, 1\). Let X1 be the data matrix ) Tj241.5 0  TD -0.4392  Tc 0.4392  Tw (augmented by the unity vector ) Tj138 0  TD /F0 12  Tf0  Tc 0  Tw (1) Tj6 0  TD /F1 12  Tf-0.498  Tc 0.123  Tw ( of ) Tj-385.5 -14.25  TD -0.4075  Tc 0.4075  Tw (dimension \(p, 1\) that contains only one values: X1=[X, ) Tj249 0  TD /F0 12  Tf0  Tc 0  Tw (1) Tj6 0  TD /F1 12  Tf-0.4124  Tc 0.2874  Tw (]. X1 is therefore of dimension ) Tj-255 -13.5  TD -0.407  Tc 0.407  Tw (\(p, n+1\). Linear regression seeks to solve the following matrix equation:) Tj321.75 0  TD 0  Tc 0  Tw ( ) Tj-321.75 -14.25  TD ( ) Tj36 0  TD ( ) Tj36 0  TD -0.22  Tc 0.22  Tw (X1 [) Tj21 0  TD /F0 12  Tf-1.08  Tc 0  Tw (w\222) Tj11.25 0  TD /F1 12  Tf-0.15  Tc 0.15  Tw (; b] = ) Tj28.5 0  TD /F0 12  Tf0  Tc 0  Tw (y) Tj6 0  TD ( ) Tj5.25 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD /F1 12  Tf-0.164  Tc (\(1\)) Tj13.5 0  TD 0  Tc ( ) Tj-373.5 -13.5  TD -0.246  Tc 0.246  Tw (Where ) Tj33.75 0  TD /F0 12  Tf0.336  Tc 0  Tw (w) Tj7.5 0  TD /F1 12  Tf-0.4813  Tc 0.3563  Tw ( if the weight vector of ) Tj103.5 0  TD -0.3753  Tc 0.3753  Tw (dimension \(1, n\) and b the bias value. Prime denotes ) Tj-144.75 -13.5  TD -0.4668  Tc 0.4668  Tw (matrix transposition.) Tj90.75 0  TD 0  Tc 0  Tw ( ) Tj-90.75 -14.25  TD -0.4914  Tc 0.4914  Tw (The resulting regression machine is:) Tj157.5 0  TD 0  Tc 0  Tw ( ) Tj-85.5 -13.5  TD -0.996  Tc (f\() Tj6.75 0  TD /F0 12  Tf0  Tc (x) Tj6 0  TD /F1 12  Tf-0.132  Tc 0.132  Tw (\) = ) Tj16.5 0  TD /F0 12  Tf0  Tc 0  Tw (x) Tj6 0  TD /F1 12  Tf(.) Tj3 0  TD /F0 12  Tf-1.08  Tc (w\222) Tj11.25 0  TD /F1 12  Tf-0.006  Tc 0.006  Tw (+b, ) Tj18.75 0  TD 0  Tc 0  Tw ( ) Tj3.75 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD -0.164  Tc (\(2\)) Tj13.5 0  TD 0  Tc ( ) Tj-373.5 -14.25  TD -0.3132  Tc 0.3132  Tw (where ) Tj30.75 0  TD /F0 12  Tf0  Tc 0  Tw (x) Tj6 0  TD /F1 12  Tf-0.4029  Tc 0.4029  Tw ( is an input pattern of dimension \(1, n\). In the case of classification, vector ) Tj333.75 0  TD /F0 12  Tf0  Tc 0  Tw (y) Tj6 0  TD /F1 12  Tf( ) Tj-376.5 -14.25  TD -0.4056  Tc 0.4056  Tw (contains binary values \(e.g. ) Tj124.5 0  TD /F2 12  Tf0.162  Tc 0  Tw (\261) Tj6.75 0  TD /F1 12  Tf-0.3768  Tc 0.3018  Tw (1, we discuss target values in mode details later\). Function ) Tj-131.25 -14.25  TD -0.996  Tc 0  Tw (f\() Tj6.75 0  TD /F0 12  Tf0  Tc (x) Tj6 0  TD /F1 12  Tf-0.4517  Tc 0.4076  Tw (\) is then used as a linear discriminant, the sign of which is used to classify pattern ) Tj363 0  TD /F0 12  Tf0  Tc 0  Tw (x) Tj6 0  TD /F1 12  Tf(.) Tj3 0  TD ( ) Tj-384.75 -13.5  TD -0.3731  Tc 0.3731  Tw (The best set of parameters in the least square sense is given by:) Tj283.5 0  TD 0  Tc 0  Tw ( ) Tj-283.5 -13.5  TD ( ) Tj36 0  TD ( ) Tj36 0  TD -0.246  Tc ([) Tj3.75 0  TD /F0 12  Tf-1.08  Tc (w\222) Tj11.25 0  TD /F1 12  Tf-0.294  Tc 0.294  Tw (; b] = X1) Tj42.75 5.25  TD /F1 8.25  Tf-0.153  Tc 0  Tw (+) Tj4.5 -5.25  TD /F1 12  Tf0  Tc ( ) Tj3 0  TD /F0 12  Tf(y) Tj6 0  TD /F1 12  Tf( ) Tj3 0  TD ( ) Tj33.75 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD -0.164  Tc (\(3\)) Tj13.5 0  TD 0  Tc ( ) Tj-373.5 -14.25  TD -0.205  Tc 0.205  Tw (where X) Tj39 0  TD 0  Tc 0  Tw (1) Tj6 5.25  TD /F1 8.25  Tf-0.153  Tc (+) Tj4.5 -5.25  TD /F1 12  Tf-0.3104  Tc 0.3104  Tw ( is the pseudo) Tj62.25 0  TD -0.246  Tc 0  Tw (-) Tj4.5 0  TD -0.391  Tc 0.391  Tw (inverse of X1. There are many ways of computing the pseudo) Tj277.5 0  TD -0.246  Tc 0  Tw (-) Tj-393.75 -13.5  TD -0.4156  Tc 0.4156  Tw (inverse, but they never require computationally more than what is required to invert a ) Tj0 -14.25  TD -0.4185  Tc 0.3685  Tw (matrix of dimension \(p, p\) or \(n, n\), whichever is smallest. \(For an introduction on ) TjETendstreamendobj11 0 obj9219endobj4 0 obj<</Type /Page/Parent 5 0 R/Resources <</Font <</F0 6 0 R /F1 8 0 R /F2 12 0 R >>/ProcSet 2 0 R>>/Contents 10 0 R>>endobj15 0 obj<</Length 16 0 R>>stream
BT90 709.5  TD0 0 0 rg /F1 12  Tf-0.166  Tc 0  Tw (pseudo) Tj33 0  TD -0.246  Tc (-) Tj4.5 0  TD -0.336  Tc (i) Tj2.25 0  TD -0.306  Tc 0.306  Tw (nverse, see: Regression and the Moore) Tj175.5 0  TD -0.246  Tc 0  Tw (-) Tj4.5 0  TD -0.1722  Tc 0.1722  Tw (Penrose pseudo) Tj72.75 0  TD -0.246  Tc 0  Tw (-) Tj4.5 0  TD -0.3016  Tc 0.3016  Tw (inverse, Academic Press, ) Tj-297 -13.5  TD -0.041  Tc 0.041  Tw (1972\). ) Tj33.75 0  TD 0  Tc 0  Tw ( ) Tj-33.75 -14.25  TD -0.4424  Tc 0.4424  Tw (We recommend to define one of the two following matrices \(whichever is smaller\):) Tj369.75 0  TD 0  Tc 0  Tw ( ) Tj-333.75 -14.25  TD -0.1905  Tc 0.1905  Tw (G = X1\222 X1 + ) Tj69 0  TD /F2 12  Tf0.072  Tc 0  Tw (d) Tj6.75 0  TD /F1 12  Tf-0.246  Tc 0.246  Tw ( I) Tj6.75 0  TD 0  Tc 0  Tw ( ) Tj25.5 0  TD ( ) Tj36 0  TD -0.3711  Tc 0.3711  Tw (of dim \(n+1, n+1\)) Tj81 0  TD 0  Tc 0  Tw ( ) Tj27 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD -0.164  Tc (\(4\)) Tj13.5 0  TD 0  Tc ( ) Tj-373.5 -15  TD -0.123  Tc (or) Tj9.75 0  TD 0  Tc ( ) Tj26.25 0  TD -0.0968  Tc 0.0968  Tw (K = X1 X1\222 + ) Tj69.75 0  TD /F2 12  Tf0.072  Tc 0  Tw (d) Tj6.75 0  TD /F1 12  Tf-0.246  Tc 0.246  Tw ( I) Tj6.75 0  TD 0  Tc 0  Tw ( ) Tj24.75 0  TD ( ) Tj36 0  TD -0.366  Tc 0.366  Tw (of dim \(p, p\)) Tj57 0  TD 0  Tc 0  Tw ( ) Tj15 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD -0.164  Tc (\(5\)) Tj13.5 0  TD 0  Tc ( ) Tj-373.5 -14.25  TD -0.3132  Tc (where) Tj27.75 0  TD -0.518  Tc 0.518  Tw ( I is the identity matrix of the right dimension. The coefficient ) Tj273 0  TD /F2 12  Tf0.072  Tc 0  Tw (d) Tj6.75 0  TD /F1 12  Tf-0.303  Tc 0.303  Tw ( \(ridge\) can be ) Tj-307.5 -15  TD -0.4513  Tc 0.4513  Tw (chosen large enough that the matrix is invertible \(e.g. ) Tj237.75 0  TD /F2 12  Tf0.246  Tc 0  Tw (d=10) Tj25.5 5.25  TD /F2 8.25  Tf0.2402  Tc (-10) Tj13.5 -5.25  TD /F2 12  Tf-0.123  Tc 0.123  Tw (\). ) Tj9.75 0  TD /F1 12  Tf-0.4472  Tc 0.4472  Tw (The method to optimize ) Tj109.5 0  TD /F2 12  Tf0.072  Tc 0  Tw (d) Tj6.75 0  TD /F1 12  Tf0  Tc ( ) Tj-402.75 -15  TD -0.3452  Tc 0.3452  Tw (is discussed later. The \223true\224 pseudo) Tj164.25 0  TD -0.246  Tc 0  Tw (-) Tj4.5 0  TD -0.5315  Tc 0.5315  Tw (inverse is obtained in the limit of  ) Tj149.25 0  TD /F2 12  Tf0.072  Tc 0  Tw (d) Tj6.75 0  TD /F1 12  Tf-0.6672  Tc 0.6672  Tw ( going) Tj27 0  TD -0.2126  Tc 0.2126  Tw ( to zero, ) Tj-351.75 -14.25  TD -0.2828  Tc 0.2828  Tw (but better predictors are obtained with non) Tj192.75 0  TD -0.246  Tc 0  Tw (-) Tj4.5 0  TD -0.592  Tc 0.342  Tw (negligible values of ) Tj87 0  TD /F2 12  Tf0.411  Tc -0.411  Tw (d, ) Tj12.75 0  TD /F1 12  Tf-0.2725  Tc 0.2725  Tw (for reasons that are ) Tj-297 -14.25  TD -0.4611  Tc 0.4611  Tw (well understood by regularization/learning theory.) Tj220.5 0  TD 0  Tc 0  Tw ( ) Tj-220.5 -13.5  TD -0.3086  Tc 0.3086  Tw (Then, the pseudo) Tj78 0  TD -0.246  Tc 0  Tw (-) Tj4.5 0  TD -0.366  Tc 0.366  Tw (inverse is computed as:) Tj104.25 0  TD 0  Tc 0  Tw ( ) Tj-186.75 -14.25  TD ( ) Tj36 0  TD -0.582  Tc (X1) Tj14.25 5.25  TD /F1 8.25  Tf-0.153  Tc (+) Tj4.5 -5.25  TD /F1 12  Tf0.159  Tc -0.159  Tw ( = G) Tj21 5.25  TD /F1 8.25  Tf0.2527  Tc 0  Tw (-) Tj3 0  TD 0.375  Tc (1) Tj4.5 -5.25  TD /F1 12  Tf-0.22  Tc 0.22  Tw ( X1\222) Tj21 0  TD 0  Tc 0  Tw ( ) Tj3.75 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD -0.164  Tc (\(6\)) Tj13.5 0  TD 0  Tc ( ) Tj-373.5 -13.5  TD -0.123  Tc (or) Tj9.75 0  TD 0  Tc ( ) Tj26.25 0  TD -0.582  Tc (X1) Tj14.25 5.25  TD /F1 8.25  Tf-0.153  Tc (+) Tj4.5 -5.25  TD /F1 12  Tf-0.0684  Tc 0.0684  Tw ( = X1\222 K) Tj42.75 5.25  TD /F1 8.25  Tf0.2527  Tc 0  Tw (-) Tj3 0  TD 0.375  Tc (1) Tj4.5 -5.25  TD /F1 12  Tf0  Tc (.) Tj3 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD -0.164  Tc (\(7\)) Tj13.5 0  TD 0  Tc ( ) Tj-373.5 -13.5  TD -0.4713  Tc 0.4713  Tw (The follow) Tj48 0  TD -0.3705  Tc 0.3705  Tw (ing theorem \(see Albert\222s book\) allows in fact to use either formula \(6\) or \(7\) ) Tj-48 -14.25  TD -0.2771  Tc 0.2771  Tw (provided that the pseudo) Tj112.5 0  TD -0.246  Tc 0  Tw (-) Tj4.5 0  TD -0.3841  Tc 0.3841  Tw (inverse \(not the true inverse\) of G or K is taken:) Tj215.25 0  TD 0  Tc 0  Tw ( ) Tj-332.25 -13.5  TD ( ) Tj36 0  TD -0.582  Tc (X1) Tj14.25 5.25  TD /F1 8.25  Tf-0.153  Tc (+) Tj4.5 -5.25  TD /F1 12  Tf0.159  Tc -0.159  Tw ( = G) Tj21 5.25  TD /F1 8.25  Tf-0.153  Tc 0  Tw (+) Tj4.5 -5.25  TD /F1 12  Tf-0.1253  Tc 0.1253  Tw ( X1\222 = X1\222 K) Tj63.75 5.25  TD /F1 8.25  Tf-0.153  Tc 0  Tw (+) Tj4.5 -5.25  TD /F1 12  Tf0  Tc (.) Tj3 0  TD ( ) Tj28.5 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD -0.164  Tc (\(8\)) Tj13.5 0  TD 0  Tc ( ) Tj-373.5 -14.25  TD -0.4782  Tc 0.4782  Tw (This not always computationally interesting, but it will become han) Tj295.5 0  TD -0.5087  Tc 0.5087  Tw (dy in what follows for ) Tj-295.5 -13.5  TD -0.2229  Tc 0.2229  Tw (other reasons.) Tj63.75 0  TD 0  Tc 0  Tw ( ) Tj-63.75 -13.5  TD -0.3809  Tc 0.3809  Tw (Formula \(8\) shows that this method can easily be turned into a kernel method. ) Tj0 -14.25  TD -0.4176  Tc 0.4176  Tw (Substituting \(8\) into \(3\) and applying it to \(2\), we get the following predictor:) Tj343.5 0  TD 0  Tc 0  Tw ( ) Tj-343.5 -13.5  TD ( ) Tj36 0  TD -0.996  Tc (f\() Tj6.75 0  TD /F0 12  Tf0  Tc (x) Tj6 0  TD /F1 12  Tf-0.17  Tc 0.17  Tw (\) = [) Tj20.25 0  TD /F0 12  Tf0  Tc 0  Tw (x) Tj6 0  TD /F1 12  Tf-0.0814  Tc 0.0814  Tw (, 1] X1\222 K) Tj48.75 5.25  TD /F1 8.25  Tf-0.153  Tc 0.3405  Tw (+ ) Tj6.75 -5.25  TD /F0 12  Tf0  Tc 0  Tw (y ) Tj9 0  TD ( ) Tj4.5 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD /F1 12  Tf-0.164  Tc (\(9\)) Tj13.5 0  TD 0  Tc ( ) Tj-373.5 -14.25  TD -0.2929  Tc 0.2929  Tw (that we can re) Tj63.75 0  TD -0.246  Tc 0  Tw (-) Tj4.5 0  TD -0.3437  Tc 0.3437  Tw (write as) Tj35.25 0  TD -0.336  Tc 0  Tw (:) Tj2.25 0  TD /F0 12  Tf0  Tc ( ) Tj-69.75 -13.5  TD /F1 12  Tf-0.996  Tc (f\() Tj6.75 0  TD /F0 12  Tf0  Tc (x) Tj6 0  TD /F1 12  Tf-0.246  Tc 0.246  Tw (\) ) Tj6.75 0  TD /F0 12  Tf-0.006  Tc 0.006  Tw (= k) Tj16.5 0  TD /F1 12  Tf-0.246  Tc 0  Tw (\() Tj3.75 0  TD /F0 12  Tf0  Tc (x) Tj6 0  TD /F1 12  Tf-0.052  Tc 0.052  Tw (\)\222 K) Tj19.5 5.25  TD /F1 8.25  Tf-0.153  Tc 0.3405  Tw (+ ) Tj6.75 -5.25  TD /F0 12  Tf0  Tc 0  Tw (y) Tj6 0  TD ( ) Tj30 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD /F1 12  Tf-0.123  Tc (\(10\)) Tj19.5 0  TD 0  Tc ( ) Tj-379.5 -13.5  TD -0.3132  Tc 0.3132  Tw (where ) Tj30.75 0  TD /F0 12  Tf0.078  Tc 0  Tw (k) Tj6.75 0  TD /F1 12  Tf-0.246  Tc (\() Tj3.75 0  TD /F0 12  Tf0  Tc (x) Tj6 0  TD /F1 12  Tf-0.3093  Tc 0.3093  Tw (\) is a \(p, 1\) dimensional vector whose components are dot products of [) Tj324 0  TD /F0 12  Tf0  Tc 0  Tw (x) Tj6 0  TD /F1 12  Tf-0.082  Tc 0.082  Tw (, 1] ) Tj-377.25 -14.25  TD -0.3952  Tc 0.3952  Tw (and of lines of X1. To generalize to any kernel method, it suffices to replace the dot ) Tj0 -13.5  TD -0.3634  Tc 0.3634  Tw (product by another kernel function k\() Tj168 0  TD /F0 12  Tf0  Tc 0  Tw (x) Tj6 0  TD /F1 12  Tf(, ) Tj6 0  TD /F0 12  Tf(x) Tj6 0  TD /F1 12  Tf-0.3315  Tc 0.3315  Tw (\222\). Then, ) Tj42 0  TD 0  Tc 0  Tw ( ) Tj-228 -14.25  TD ( ) Tj36 0  TD -0.0348  Tc 0.0348  Tw (K = [k\() Tj35.25 0  TD /F0 12  Tf0  Tc 0  Tw (x) Tj6 -1.5  TD /F1 8.25  Tf-0.0435  Tc (i) Tj2.25 1.5  TD /F1 12  Tf0  Tc (, ) Tj6 0  TD /F0 12  Tf(x) Tj6 -1.5  TD /F1 8.25  Tf-0.0435  Tc (j) Tj2.25 1.5  TD /F1 12  Tf-0.246  Tc (\)]) Tj7.5 0  TD 0  Tc ( ) Tj6.75 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD -0.123  Tc (\(11\)) Tj19.5 0  TD 0  Tc ( ) Tj-379.5 -13.5  TD -0.4314  Tc 0.4314  Tw (where indices i and j run from 1 to p \(all training patterns\), and) Tj280.5 0  TD 0  Tc 0  Tw ( ) Tj-280.5 -13.5  TD ( ) Tj36 0  TD /F0 12  Tf0.078  Tc (k) Tj6.75 0  TD /F1 12  Tf-0.246  Tc (\() Tj3.75 0  TD /F0 12  Tf0  Tc (x) Tj6 0  TD /F1 12  Tf-0.1275  Tc 0.1275  Tw (\) = k\() Tj26.25 0  TD /F0 12  Tf0  Tc 0  Tw (x) Tj6 0  TD /F1 12  Tf(, ) Tj6 0  TD /F0 12  Tf(x) Tj6 -1.5  TD /F1 8.25  Tf-0.0435  Tc (i) Tj2.25 1.5  TD /F1 12  Tf-0.246  Tc (\)) Tj3.75 0  TD 0  Tc ( ) Tj5.25 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD -0.123  Tc (\(12\)) Tj19.5 0  TD 0  Tc ( ) Tj-379.5 -14.25  TD -0.4123  Tc 0.4123  Tw (where index i  runs from 1 to p.) Tj141.75 0  TD 0  Tc 0  Tw ( ) Tj-141.75 -13.5  TD -0.3576  Tc 0.3576  Tw (From formula \(10\) we can also make apparent that f\() Tj240 0  TD /F0 12  Tf0  Tc 0  Tw (x) Tj6 0  TD /F1 12  Tf-0.4616  Tc 0.3544  Tw (\) is a linear combination of kernel ) Tj-246 -14.25  TD -0.525  Tc 0  Tw (functions:) Tj42 0  TD 0  Tc ( ) Tj-42 -14.25  TD ( ) Tj36 0  TD -0.996  Tc (f\() Tj6.75 0  TD /F0 12  Tf0  Tc (x) Tj6 0  TD /F1 12  Tf-0.246  Tc 0.246  Tw (\) ) Tj6.75 0  TD /F0 12  Tf-0.006  Tc 0.006  Tw (= k) Tj16.5 0  TD /F1 12  Tf-0.246  Tc 0  Tw (\() Tj3.75 0  TD /F0 12  Tf0  Tc (x) Tj6 0  TD /F1 12  Tf-0.246  Tc 0.246  Tw (\)\222 ) Tj10.5 0  TD /F3 12  Tf1.287  Tc -0.537  Tw (b = ) Tj23.25 0  TD /F2 12  Tf-0.354  Tc 0  Tw (S) Tj7.5 -1.5  TD /F1 8.25  Tf-0.0435  Tc (i) Tj2.25 1.5  TD /F2 12  Tf0.162  Tc -0.162  Tw ( b) Tj10.5 -1.5  TD /F1 8.25  Tf-0.0435  Tc 0  Tw (i) Tj2.25 1.5  TD /F1 12  Tf-0.123  Tc 0.123  Tw ( k\() Tj12.75 0  TD /F0 12  Tf0  Tc 0  Tw (x) Tj6 0  TD /F1 12  Tf(, ) Tj6 0  TD /F0 12  Tf(x) Tj6 -1.5  TD /F1 8.25  Tf-0.0435  Tc (i) Tj2.25 1.5  TD /F1 12  Tf-0.246  Tc (\)) Tj3.75 0  TD 0  Tc ( ) Tj5.25 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD -0.123  Tc (\(13\)) Tj19.5 0  TD 0  Tc ( ) Tj-379.5 -14.25  TD -0.3132  Tc 0.3132  Tw (where ) Tj30.75 0  TD 0  Tc 0  Tw ( ) Tj5.25 -14.25  TD /F3 12  Tf1.287  Tc -0.537  Tw (b = ) Tj23.25 0  TD /F1 12  Tf0.336  Tc 0  Tw (K) Tj9 5.25  TD /F1 8.25  Tf-0.153  Tc 0.3405  Tw (+ ) Tj6.75 -5.25  TD /F0 12  Tf0  Tc 0  Tw (y) Tj6 0  TD ( ) Tj27 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD /F1 12  Tf-0.123  Tc (\(14\)) Tj19.5 0  TD /F0 12  Tf0  Tc ( ) Tj-379.5 -14.25  TD /F1 12  Tf( ) Tj0 -13.5  TD -0.4196  Tc 0.3696  Tw (The connection to Fisher\222s linear discriminant may not be obvious, but in can be shown ) TjT* -0.4227  Tc 0.4227  Tw (\(Duda and Hart, 1973\) that using this method is equivalent to Fisher\222s linear discriminant ) Tj0 -15  TD -0.543  Tc 0.543  Tw (in th) Tj18.75 0  TD -0.4389  Tc 0.4389  Tw (e following case: class target values  ) Tj164.25 0  TD /F2 12  Tf0.162  Tc 0  Tw (\261) Tj6.75 0  TD /F1 12  Tf-0.1842  Tc 0.1842  Tw (1 are replaced by p/p) Tj97.5 -1.5  TD /F1 8.25  Tf0.375  Tc 0  Tw (1) Tj4.5 1.5  TD /F1 12  Tf-0.276  Tc 0.276  Tw ( and ) Tj22.5 0  TD 0  Tc 0  Tw (\226) Tj6 0  TD -0.112  Tc (p/p) Tj15 -1.5  TD /F1 8.25  Tf0.375  Tc (2) Tj4.5 1.5  TD /F1 12  Tf-0.2237  Tc 0.2237  Tw (, where p) Tj42.75 -1.5  TD /F1 8.25  Tf0.375  Tc 0  Tw (1) Tj4.5 1.5  TD /F1 12  Tf-0.276  Tc 0.276  Tw ( and ) Tj-387 -13.5  TD 0  Tc 0  Tw (p) Tj6 -1.5  TD /F1 8.25  Tf0.375  Tc (2) Tj4.5 1.5  TD /F1 12  Tf-0.3936  Tc 0.3936  Tw ( are the number of examples of the positive and negative class respectively. This does ) Tj-10.5 -15  TD -0.4146  Tc 0.4146  Tw (not always turn out to give better results than using targets ) Tj262.5 0  TD /F2 12  Tf0.162  Tc 0  Tw (\261) Tj6.75 0  TD /F1 12  Tf-0.3642  Tc 0.3642  Tw (1 when the class ) Tj77.25 0  TD -0.4735  Tc 0.4735  Tw (cardinalities ) Tj-346.5 -13.5  TD -0.3822  Tc 0.3822  Tw (are different.) Tj57 0  TD 0  Tc 0  Tw ( ) Tj-57 -14.25  TD -0.2986  Tc 0.2986  Tw (The kernel regression method described above has been re) Tj266.25 0  TD -0.246  Tc 0  Tw (-) Tj4.5 0  TD -0.342  Tc 0.342  Tw (discovered several times and ) Tj-270.75 -13.5  TD -0.4063  Tc 0.3126  Tw (is known under various names \(e.g. \223ridge regression\224 ) Tj245.25 0  TD -0.22  Tc 0  Tw (\(Hoerl) Tj29.25 0  TD -0.246  Tc (-) Tj4.5 0  TD -0.2265  Tc (Kennard\)) Tj43.5 0  TD -0.5141  Tc 0.1391  Tw (, \223regularization ) Tj-322.5 -14.25  TD -0.3953  Tc 0.3953  Tw (networks\224 \(Poggio) Tj84.75 0  TD -0.246  Tc 0  Tw (-) Tj4.5 0  TD -0.4661  Tc 0.4661  Tw (Girosi\), neural network \223weight) Tj141 0  TD -0.246  Tc 0  Tw (-) Tj4.5 0  TD -0.0585  Tc (deca) Tj21.75 0  TD -0.4807  Tc 0.2932  Tw (y\224, see historical in ) Tj-256.5 -13.5  TD 0 0 1 rg -0.3258  Tc 0  Tw (http://www.anc.ed.ac.uk/~mjo/intro/node19.html) TjET90 77.25 220.5 0.75 re fBT310.5 79.5  TD0 0 0 rg -0.3363  Tc 0.3363  Tw (\). Since it is over 20 year) Tj113.25 0  TD -0.246  Tc 0  Tw (-) Tj4.5 0  TD -0.389  Tc 0.389  Tw (old one should ) TjETendstreamendobj16 0 obj13329endobj14 0 obj<</Type /Page/Parent 5 0 R/Resources <</Font <</F0 6 0 R /F1 8 0 R /F2 12 0 R /F3 17 0 R >>/ProcSet 2 0 R>>/Contents 15 0 R>>endobj20 0 obj<</Length 21 0 R>>stream
BT90 709.5  TD0 0 0 rg /F1 12  Tf-0.3499  Tc 0.3499  Tw (not worry about patent infringement. Unlike SVMs it does not provide a sparse sol) Tj372.75 0  TD -0.5844  Tc -0.1656  Tw (ution ) Tj-372.75 -13.5  TD -0.3918  Tc 0.3918  Tw (\(the sum runs over all the examples, not only on the support vectors. But it often provides ) Tj0 -14.25  TD -0.423  Tc 0.423  Tw (similar performance and may be faster to train \(depending on implementations.\)) Tj355.5 0  TD 0  Tc 0  Tw ( ) Tj-355.5 -13.5  TD ( ) Tj0 -14.25  TD -0.4906  Tc 0.4906  Tw (Ridge optimization) TjET90 651.75 83.25 0.75 re fBT173.25 654  TD0  Tc 0  Tw ( ) Tj-83.25 -13.5  TD -0.4426  Tc 0.3744  Tw (Most of the computational time is spent to invert matrix G ) Tj261.75 0  TD -0.249  Tc 0.249  Tw (or K \(Equations 4 and 5\), ) Tj-261.75 -13.5  TD -0.4594  Tc 0.4594  Tw (whichever is smallest. To optimize the ridge, one must perform such inversion for ) Tj0 -15  TD -0.4656  Tc 0.2156  Tw (various values of ) Tj78 0  TD /F2 12  Tf0.411  Tc -0.411  Tw (d. ) Tj12.75 0  TD /F1 12  Tf-0.4328  Tc 0.3578  Tw (Fortunately, this can be performed efficiently: One must perform an ) Tj-90.75 -13.5  TD -0.328  Tc 0.328  Tw (eigen value decomposition of G or K, e.g. for K:) Tj219 0  TD 0  Tc 0  Tw ( ) Tj-219 -14.25  TD ( ) Tj36 0  TD -0.1848  Tc (K=UDV) Tj40.5 0  TD 0  Tc ( ) Tj31.5 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD -0.123  Tc (\(15\)) Tj19.5 0  TD 0  Tc ( ) Tj-343.5 -13.5  TD -0.3904  Tc 0.3904  Tw (where U and V are orthogonal matrices and D is a diagonal matrix of eigen vectors.) Tj375.75 0  TD 0  Tc 0  Tw ( ) Tj-375.75 -14.25  TD -0.3612  Tc 0.3612  Tw (The inverse of K is obtained as:) Tj142.5 0  TD 0  Tc 0  Tw ( ) Tj-142.5 -13.5  TD ( ) Tj36 0  TD 0.336  Tc (K) Tj9 5.25  TD /F1 8.25  Tf0.2527  Tc (-) Tj3 0  TD 0.375  Tc (1) Tj4.5 -5.25  TD /F1 12  Tf-0.273  Tc (=V\222D) Tj27 5.25  TD /F1 8.25  Tf0.2527  Tc (-) Tj3 0  TD 0.375  Tc (1) Tj4.5 -5.25  TD /F1 12  Tf-0.33  Tc (U\222) Tj12 0  TD 0  Tc ( ) Tj9 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD -0.123  Tc (\(16\)) Tj19.5 0  TD 0  Tc ( ) Tj-343.5 -14.25  TD -0.4879  Tc 0.4879  Tw (Conveniently, if we call K) Tj116.25 -1.5  TD /F1 8.25  Tf0.375  Tc 0  Tw (0) Tj4.5 1.5  TD /F1 12  Tf-0.4282  Tc 0.4282  Tw ( the matrix with a ridge of zero, the inverse of K) Tj215.25 -1.5  TD /F2 8.25  Tf-0.3255  Tc 0  Tw (d) Tj3.75 1.5  TD /F1 12  Tf0.159  Tc -0.159  Tw ( = K) Tj21.75 -1.5  TD /F1 8.25  Tf0.375  Tc 0  Tw (0) Tj4.5 1.5  TD /F1 12  Tf-0.018  Tc 0.018  Tw ( + ) Tj12.75 0  TD /F2 12  Tf0.072  Tc 0  Tw (d) Tj6.75 0  TD /F1 12  Tf-0.246  Tc 0.246  Tw ( I ) Tj-385.5 -14.25  TD -0.2372  Tc 0.2372  Tw (can be computed as) Tj90.75 0  TD -0.336  Tc 0  Tw (:) Tj2.25 0  TD 0  Tc ( ) Tj-93 -14.25  TD ( ) Tj36 0  TD 0.336  Tc (K) Tj9 -1.5  TD /F2 8.25  Tf-0.3255  Tc (d) Tj3.75 6.75  TD /F1 8.25  Tf0.2527  Tc (-) Tj3 0  TD 0.375  Tc (1) Tj4.5 -5.25  TD /F1 12  Tf-0.351  Tc (=V\222\(D+) Tj37.5 0  TD /F2 12  Tf0.072  Tc (d) Tj6.75 0  TD /F1 12  Tf-0.246  Tc 0.246  Tw ( I\)) Tj10.5 5.25  TD /F1 8.25  Tf0.2527  Tc 0  Tw (-) Tj3 0  TD 0.375  Tc (1) Tj4.5 -5.25  TD /F1 12  Tf-0.33  Tc (U\222) Tj12 0  TD 0  Tc ( ) Tj-130.5 -14.25  TD -0.205  Tc 0.205  Tw (where K) Tj39.75 -1.5  TD /F1 8.25  Tf0.375  Tc 0  Tw (0) Tj4.5 1.5  TD /F1 12  Tf-0.4782  Tc 0.4157  Tw (=UDV. The inversion of a diagonal matrix being trivial, once the eigen ) Tj-44.25 -13.5  TD -0.333  Tc 0.333  Tw (decomposition of K) Tj90 -1.5  TD /F1 8.25  Tf0.375  Tc 0  Tw (0) Tj4.5 1.5  TD /F1 12  Tf-0.4477  Tc 0.4477  Tw ( is performed, obtaining solutions for various values of the ridge is ) Tj-94.5 -14.25  TD -0.4645  Tc 0  Tw (inexpensive.) Tj54.75 0  TD 0  Tc ( ) Tj-54.75 -13.5  TD -0.4698  Tc 0.4698  Tw (Optimizing the ridge is rendered further efficient by) Tj227.25 0  TD -0.498  Tc 0.498  Tw ( the leave) Tj42.75 0  TD -0.246  Tc 0  Tw (-) Tj4.5 0  TD -0.526  Tc (one) Tj16.5 0  TD -0.246  Tc (-) Tj4.5 0  TD -0.444  Tc 0.444  Tw (out calculations ) Tj-295.5 -14.25  TD -0.4641  Tc 0.4641  Tw (explained in the next section.) Tj129 0  TD 0  Tc 0  Tw ( ) Tj-129 -13.5  TD ( ) Tj0 -13.5  TD -0.4632  Tc (Leave) Tj27.75 0  TD -0.246  Tc (-) Tj4.5 0  TD -0.526  Tc (one) Tj16.5 0  TD -0.246  Tc (-) Tj4.5 0  TD -0.494  Tc 0.494  Tw (out calculations) TjET90 401.25 122.25 0.75 re fBT212.25 403.5  TD0  Tc 0  Tw ( ) Tj-122.25 -14.25  TD -0.3038  Tc 0.3038  Tw (The performance of a predictor can be assessed in a number of ways. For regression, one ) Tj0 -13.5  TD -0.423  Tc 0.423  Tw (typically computes the mean) Tj126.75 0  TD -0.246  Tc 0  Tw (-) Tj4.5 0  TD -0.22  Tc (square) Tj30 0  TD -0.246  Tc (-) Tj4.5 0  TD -0.2705  Tc 0.2705  Tw (error \(mse\):) Tj53.25 0  TD 0  Tc 0  Tw ( ) Tj-147 -15  TD -0.3375  Tc 0.3375  Tw (mse = ) Tj30.75 0  TD /F2 12  Tf0.162  Tc 0  Tw (<) Tj6.75 0  TD /F1 12  Tf-0.498  Tc 0.123  Tw ( \(y ) Tj15 0  TD 0  Tc 0  Tw (\226) Tj6 0  TD -0.873  Tc 0.873  Tw ( \377\)) Tj12 5.25  TD /F1 8.25  Tf0.375  Tc 0  Tw (2) Tj4.5 -5.25  TD /F1 12  Tf0  Tc ( ) Tj3 0  TD /F2 12  Tf0.162  Tc (>) Tj6.75 0  TD 0  Tc ( ) Tj23.25 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD -0.123  Tc (\(17\)) Tj19.5 0  TD 0  Tc ( ) Tj-343.5 -13.5  TD /F1 12  Tf0.078  Tc (F) Tj6.75 0  TD -0.3341  Tc 0.3341  Tw (or classification, one computes the error rate:) Tj202.5 0  TD 0  Tc 0  Tw ( ) Tj-173.25 -15  TD ( ) Tj36 0  TD -0.1543  Tc 0.1543  Tw (errate = ) Tj39 0  TD /F2 12  Tf0.162  Tc 0  Tw (<) Tj6.75 0  TD /F1 12  Tf0  Tc ( ) Tj3 0  TD /F2 12  Tf0.108  Tc (Q) Tj9 0  TD /F1 12  Tf-0.246  Tc (\() Tj3.75 0  TD (-) Tj4.5 0  TD -0.582  Tc 0.582  Tw (y\377\) ) Tj17.25 0  TD /F2 12  Tf0.162  Tc 0  Tw (>) Tj6.75 0  TD 0  Tc ( ) Tj18 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD -0.123  Tc (\(18\)) Tj19.5 0  TD 0  Tc ( ) Tj-343.5 -14.25  TD /F1 12  Tf-0.3132  Tc 0.3132  Tw (where ) Tj30.75 0  TD /F2 12  Tf0.108  Tc 0  Tw (Q) Tj9 0  TD /F1 12  Tf-0.4236  Tc 0.4236  Tw (\(.\)  denotes the function that has value 1 if the argument is positive and zero ) Tj-39.75 -14.25  TD -0.3156  Tc 0  Tw (otherwise.) Tj46.5 0  TD 0  Tc ( ) Tj-46.5 -14.25  TD -0.4315  Tc 0.0565  Tw (The notation ) Tj59.25 0  TD /F2 12  Tf0.162  Tc 0  Tw (<) Tj6.75 0  TD /F1 12  Tf0  Tc ( . ) Tj9 0  TD /F2 12  Tf0.162  Tc -0.162  Tw (> ) Tj9.75 0  TD /F1 12  Tf-0.3449  Tc 0.3449  Tw (indicates that an average is taken over a number of patterns.) Tj270 0  TD -0.4812  Tc 0.4812  Tw ( If the ) Tj-354.75 -14.25  TD -0.3674  Tc 0.3138  Tw (average is taken over the training examples used to adjust the parameters, the estimation ) Tj0 -13.5  TD -0.3417  Tc 0.3417  Tw (of performance is biased \(lower mse and error rate than on unseen examples\). Therefore ) Tj0 -14.25  TD -0.3432  Tc 0.3432  Tw (it is preferable to use unseen test examples to estimate performance. H) Tj317.25 0  TD -0.3969  Tc 0.3969  Tw (owever, this is ) Tj-317.25 -13.5  TD -0.319  Tc 0.319  Tw (often impractical because examples are scarce. ) Tj214.5 0  TD 0  Tc 0  Tw ( ) Tj-214.5 -13.5  TD -0.373  Tc 0.373  Tw (An alternative method is to use cross) Tj165.75 0  TD -0.246  Tc 0  Tw (-) Tj4.5 0  TD -0.4066  Tc 0.4066  Tw (validation, that is to train on a subset of examples ) Tj-170.25 -14.25  TD -0.4138  Tc 0.4138  Tw (and test on the rest and rotate through the examples many times. In the limit, one trains ) Tj0 -13.5  TD -0.5205  Tc 0.5205  Tw (on all the ) Tj43.5 0  TD -0.4051  Tc 0.4051  Tw (examples but one and test on the remaining example. The predictions on the ) Tj-43.5 -14.25  TD -0.3317  Tc 0.3317  Tw (examples held out are used to assess performance \(leave) Tj255.75 0  TD -0.246  Tc 0  Tw (-) Tj4.5 0  TD -0.526  Tc (one) Tj16.5 0  TD -0.246  Tc (-) Tj4.5 0  TD -0.3256  Tc 0.3256  Tw (out method.\)) Tj57.75 0  TD 0  Tc 0  Tw ( ) Tj-339 -13.5  TD -0.5288  Tc 0.5288  Tw (The leave) Tj43.5 0  TD -0.246  Tc 0  Tw (-) Tj4.5 0  TD -0.526  Tc (one) Tj16.5 0  TD -0.246  Tc (-) Tj4.5 0  TD -0.3648  Tc 0.3648  Tw (out method is computationally expensive because p models need to be ) Tj-69 -13.5  TD -0.3947  Tc 0.3947  Tw (trained. Fortunately, in t) Tj107.25 0  TD -0.321  Tc 0.321  Tw (he case of the least) Tj85.5 0  TD -0.246  Tc 0  Tw (-) Tj4.5 0  TD -0.3758  Tc 0.3758  Tw (square linear model, there are formulas to ) Tj-197.25 -14.25  TD -0.4208  Tc 0.4208  Tw (compute the leave) Tj81.75 0  TD -0.246  Tc 0  Tw (-) Tj4.5 0  TD -0.526  Tc (one) Tj16.5 0  TD -0.246  Tc (-) Tj4.5 0  TD -0.4629  Tc 0.4004  Tw (out statistics without having to train more than one model on all ) Tj-107.25 -13.5  TD -0.374  Tc 0.374  Tw (the examples.) Tj61.5 0  TD 0  Tc 0  Tw ( ) Tj-61.5 -14.25  TD -0.4609  Tc 0.4609  Tw (Let us call XX) Tj65.25 5.25  TD /F1 8.25  Tf-0.153  Tc 0  Tw (+) Tj4.5 -5.25  TD /F1 12  Tf-0.3503  Tc 0.2926  Tw ( the projection matrix onto the subspace spanned by the column vectors of ) Tj-69.75 -13.5  TD 0.336  Tc 0  Tw (X) Tj8.25 0  TD -0.4838  Tc 0.4838  Tw ( \(for simplicity we leave out the ) Tj143.25 0  TD /F0 12  Tf0  Tc 0  Tw (1) Tj6 0  TD /F1 12  Tf-0.4009  Tc 0.4009  Tw ( column used to set the bias value\). Let us call r) Tj213 -1.5  TD /F1 8.25  Tf-0.0435  Tc 0  Tw (i) Tj2.25 1.5  TD /F1 12  Tf-0.375  Tc 0.375  Tw ( the i) Tj21.75 5.25  TD /F1 8.25  Tf0.5407  Tc 0  Tw (th) Tj7.5 -5.25  TD /F1 12  Tf0  Tc ( ) Tj-402 -13.5  TD -0.5087  Tc (residual:) Tj36.75 0  TD 0  Tc ( ) TjETendstreamendobj21 0 obj9716endobj19 0 obj<</Type /Page/Parent 5 0 R/Resources <</Font <</F0 6 0 R /F1 8 0 R /F2 12 0 R >>/ProcSet 2 0 R>>/Contents 20 0 R>>endobj23 0 obj<</Length 24 0 R>>stream
BT144 709.5  TD0 0 0 rg /F1 12  Tf-0.246  Tc 0  Tw (r) Tj3.75 -1.5  TD /F1 8.25  Tf-0.0435  Tc 0.231  Tw (i ) Tj4.5 1.5  TD /F1 12  Tf-0.009  Tc 0.009  Tw (= y) Tj15 -1.5  TD /F1 8.25  Tf-0.0435  Tc 0  Tw (i) Tj2.25 1.5  TD /F1 12  Tf0  Tc ( ) Tj3 0  TD (\226) Tj6 0  TD ( \377) Tj8.25 -1.5  TD /F1 8.25  Tf-0.0435  Tc (i) Tj2.25 1.5  TD /F1 12  Tf0  Tc ( ) Tj9 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD -0.123  Tc (\(19\)) Tj19.5 0  TD 0  Tc ( ) Tj-379.5 -13.5  TD -0.4365  Tc 0.4365  Tw (The i) Tj22.5 5.25  TD /F1 8.25  Tf0.5407  Tc 0  Tw (th) Tj7.5 -5.25  TD /F1 12  Tf-0.564  Tc 0.564  Tw ( leave) Tj26.25 0  TD -0.246  Tc 0  Tw (-) Tj4.5 0  TD -0.526  Tc (one) Tj16.5 0  TD -0.246  Tc (-) Tj4.5 0  TD -0.4641  Tc 0.4641  Tw (out residual is given by \(Efron) Tj133.5 0  TD -0.246  Tc 0  Tw (-) Tj4.5 0  TD -0.3773  Tc 0.3773  Tw (Tibshirani, 1993\):) Tj79.5 0  TD 0  Tc 0  Tw ( ) Tj-299.25 -14.25  TD ( ) Tj36 0  TD -0.078  Tc (e) Tj5.25 -1.5  TD /F1 8.25  Tf-0.0435  Tc 0.231  Tw (i ) Tj4.5 1.5  TD /F1 12  Tf-0.132  Tc 0.132  Tw (= r) Tj13.5 -1.5  TD /F1 8.25  Tf-0.0435  Tc 0.231  Tw (i ) Tj4.5 1.5  TD /F1 12  Tf-0.194  Tc 0.194  Tw (/ [1) Tj15.75 0  TD -0.246  Tc 0  Tw (-) Tj4.5 0  TD -0.358  Tc 0.358  Tw (\( XX) Tj23.25 5.25  TD /F1 8.25  Tf-0.153  Tc 0  Tw (+) Tj4.5 -5.25  TD /F1 12  Tf-0.246  Tc (\)) Tj3.75 -1.5  TD /F1 8.25  Tf-0.0435  Tc (ii) Tj4.5 1.5  TD /F1 12  Tf-0.246  Tc (]) Tj3.75 0  TD 0  Tc ( ) Tj20.25 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD -0.123  Tc (\(20\)) Tj19.5 0  TD 0  Tc ( ) Tj-379.5 -13.5  TD -0.5288  Tc 0.5288  Tw (The leave) Tj43.5 0  TD -0.246  Tc 0  Tw (-) Tj4.5 0  TD -0.526  Tc (one) Tj16.5 0  TD -0.246  Tc (-) Tj4.5 0  TD -0.403  Tc 0.403  Tw (out mse ) Tj38.25 0  TD -0.5107  Tc 0  Tw (follows:) Tj34.5 0  TD 0  Tc ( ) Tj-141.75 -19.5  TD ( ) Tj36 0  TD -0.2511  Tc 0.2511  Tw (mse_loo = \(1/p\) ) Tj76.5 0  TD /F2 15.75  Tf-0.324  Tc 0  Tw (S) Tj9.75 -1.5  TD /F1 8.25  Tf0.1547  Tc (i=1..p) Tj20.25 1.5  TD /F1 12  Tf-0.078  Tc 0.078  Tw ( e) Tj8.25 -1.5  TD /F1 8.25  Tf-0.0435  Tc 0  Tw (i) Tj2.25 6.75  TD 0.375  Tc (2) Tj4.5 -5.25  TD /F1 12  Tf0  Tc ( ) Tj22.5 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD -0.123  Tc (\(21\)) Tj19.5 0  TD 0  Tc ( ) Tj-379.5 -14.25  TD -0.3578  Tc 0.3578  Tw (The mse_loo thus computed is exact.) Tj167.25 0  TD 0  Tc 0  Tw ( ) Tj-167.25 -13.5  TD -0.4667  Tc 0.4667  Tw (Similarly, there exists a formula to compute the leave) Tj236.25 0  TD -0.246  Tc 0  Tw (-) Tj4.5 0  TD -0.526  Tc (one) Tj16.5 0  TD -0.246  Tc (-) Tj4.5 0  TD -0.1597  Tc 0.1597  Tw (out error rate \(Opper) Tj96.75 0  TD -0.246  Tc 0  Tw (-) Tj4.5 0  TD -0.4155  Tc 0.4155  Tw (Winther, ) Tj-363 -14.25  TD -0.3629  Tc 0.3629  Tw (1999\). Such formula however is approximate, but it is a very good approximation. T) Tj379.5 0  TD -0.414  Tc 0.414  Tw (he ) Tj-379.5 -13.5  TD -0.4399  Tc 0.4399  Tw (method consists in replacing the predicted value \377 in formula \(18\) by:) Tj309 0  TD 0  Tc 0  Tw ( ) Tj-309 -14.25  TD ( ) Tj36 0  TD (\377) Tj5.25 -1.5  TD /F1 8.25  Tf0.1657  Tc 0.0218  Tw (loo i) Tj15.75 1.5  TD /F1 12  Tf-0.009  Tc 0.009  Tw ( =  \377) Tj21 -1.5  TD /F1 8.25  Tf-0.0435  Tc 0  Tw (i) Tj2.25 1.5  TD /F1 12  Tf0  Tc ( ) Tj3 0  TD (\226) Tj6 0  TD ( ) Tj3 0  TD /F2 12  Tf0.162  Tc (l) Tj6.75 -1.5  TD /F1 8.25  Tf-0.0435  Tc (i) Tj2.25 1.5  TD /F2 12  Tf0.162  Tc -0.162  Tw ( b) Tj10.5 -1.5  TD /F1 8.25  Tf-0.0435  Tc 0  Tw (i) Tj2.25 1.5  TD /F1 12  Tf0  Tc ( ) Tj30 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD -0.123  Tc (\(22\)) Tj19.5 0  TD 0  Tc ( ) Tj-379.5 -15  TD -0.3413  Tc 0.3413  Tw (where the ) Tj47.25 0  TD /F2 12  Tf0.162  Tc 0  Tw (b) Tj7.5 -1.5  TD /F1 8.25  Tf-0.0435  Tc (i) Tj2.25 1.5  TD /F1 12  Tf-0.3906  Tc 0.3906  Tw ( coefficients are obtained from [\(XX) Tj163.5 5.25  TD /F1 8.25  Tf0.2092  Tc 0  Tw (T) Tj6 -5.25  TD /F1 12  Tf-0.018  Tc (+) Tj6.75 0  TD /F2 12  Tf0.072  Tc (d) Tj6.75 0  TD /F1 12  Tf-0.246  Tc (I\)) Tj7.5 5.25  TD /F1 8.25  Tf0  Tc 0.1875  Tw ( ) Tj2.25 0  TD 0.375  Tc 0  Tw (\226) Tj4.5 0  TD (1) Tj4.5 -5.25  TD /F1 12  Tf-0.33  Tc (Y]) Tj12 -1.5  TD /F1 8.25  Tf-0.0435  Tc 0.231  Tw ( i) Tj4.5 1.5  TD /F1 12  Tf-0.2846  Tc 0.2846  Tw (, and the ) Tj42 0  TD /F2 12  Tf0.162  Tc 0  Tw (l) Tj6.75 -1.5  TD /F1 8.25  Tf-0.0435  Tc (i) Tj2.25 1.5  TD /F2 12  Tf0  Tc ( ) Tj3 0  TD /F1 12  Tf-0.4088  Tc 0.4088  Tw (coefficients are ) Tj-329.25 -15  TD -0.388  Tc 0.013  Tw (obtained from ) Tj66 0  TD 0  Tc 0  Tw (\226) Tj6 0  TD /F2 12  Tf0.072  Tc (d) Tj6.75 0  TD /F1 12  Tf-0.24  Tc (+[1/[\(XX) Tj43.5 5.25  TD /F1 8.25  Tf0.2092  Tc (T) Tj6 -5.25  TD /F1 12  Tf-0.018  Tc (+) Tj6.75 0  TD /F2 12  Tf0.072  Tc (d) Tj6.75 0  TD /F1 12  Tf-0.246  Tc (I\)) Tj7.5 5.25  TD /F1 8.25  Tf0  Tc 0.1875  Tw ( ) Tj2.25 0  TD 0.375  Tc 0  Tw (\226) Tj4.5 0  TD (1) Tj4.5 -5.25  TD /F1 12  Tf-0.246  Tc (]) Tj3.75 -1.5  TD /F1 8.25  Tf-0.0435  Tc 0.231  Tw ( ii) Tj6.75 1.5  TD /F1 12  Tf0  Tc 0  Tw (.) Tj3 0  TD ( ) Tj-174 -13.5  TD -0.5576  Tc 0.5576  Tw (The resulting leave) Tj83.25 0  TD -0.246  Tc 0  Tw (-) Tj4.5 0  TD 0  Tc (o) Tj6 0  TD -0.789  Tc (ne) Tj10.5 0  TD -0.246  Tc (-) Tj4.5 0  TD -0.2722  Tc 0.2722  Tw (out error rate is obtained as:) Tj126.75 0  TD 0  Tc 0  Tw ( ) Tj-235.5 -19.5  TD ( ) Tj36 0  TD -0.1871  Tc 0.1871  Tw (errate_loo = \(1/p\) ) Tj84.75 0  TD /F2 15.75  Tf-0.324  Tc 0  Tw (S) Tj9.75 -1.5  TD /F1 8.25  Tf0.1547  Tc (i=1..p) Tj20.25 1.5  TD /F1 12  Tf0  Tc ( ) Tj3 0  TD /F2 12  Tf0.108  Tc (Q) Tj9 0  TD /F1 12  Tf-0.246  Tc (\() Tj3.75 0  TD (-) Tj4.5 0  TD 0  Tc (y) Tj5.25 -1.5  TD /F1 8.25  Tf-0.0435  Tc (i) Tj2.25 1.5  TD /F1 12  Tf0  Tc ( \377) Tj8.25 -1.5  TD /F1 8.25  Tf0.1657  Tc 0.0218  Tw (loo i) Tj15.75 1.5  TD /F1 12  Tf-0.246  Tc 0  Tw (\)) Tj3.75 0  TD 0  Tc ( ) Tj9.75 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD ( ) Tj36 0  TD -0.123  Tc (\(10\)) Tj19.5 0  TD 0  Tc ( ) Tj-379.5 -14.25  TD ( ) Tj0 -13.5  TD -0.4341  Tc 0.4341  Tw (Noticing that XX) Tj77.25 5.25  TD /F1 8.25  Tf0.2092  Tc 0  Tw (T) Tj6 -5.25  TD /F4 12  Tf0  Tc -0.336  Tw ( ) Tj3 0  TD /F1 12  Tf-0.388  Tc 0.388  Tw (is the Gram) Tj51.75 0  TD -0.246  Tc 0  Tw (-) Tj4.5 0  TD -0.404  Tc (matrix/Hessian/kernel) Tj96.75 0  TD -0.246  Tc (-) Tj4.5 0  TD -0.3536  Tc 0.3536  Tw (matrix K of kernel least square, ) Tj-243.75 -13.5  TD -0.4018  Tc 0.4018  Tw (both formulas mse_loo and errate_loo are trivially kernelized and extend to kernel le) Tj378 0  TD -0.194  Tc 0.194  Tw (ast ) Tj-378 -14.25  TD -0.2518  Tc 0.2518  Tw (square. Formula \(20\) becomes e) Tj147.75 -1.5  TD /F1 8.25  Tf-0.0435  Tc 0.231  Tw (i ) Tj4.5 1.5  TD /F1 12  Tf-0.132  Tc 0.132  Tw (= r) Tj13.5 -1.5  TD /F1 8.25  Tf-0.0435  Tc 0.231  Tw (i ) Tj4.5 1.5  TD /F1 12  Tf-0.194  Tc 0.194  Tw (/ [1) Tj15.75 0  TD -0.246  Tc 0  Tw (-) Tj4.5 0  TD 0.142  Tc -0.142  Tw (\( KK) Tj24.75 5.25  TD /F1 8.25  Tf0.2527  Tc 0  Tw (-) Tj3 0  TD 0.375  Tc (1) Tj4.5 -5.25  TD /F1 12  Tf-0.246  Tc (\)) Tj3.75 -1.5  TD /F1 8.25  Tf-0.0435  Tc (ii) Tj4.5 1.5  TD /F1 12  Tf-0.296  Tc 0.296  Tw (]. For the others, substitute XX) Tj141 5.25  TD /F1 8.25  Tf0.2092  Tc 0  Tw (T) Tj6 -5.25  TD /F1 12  Tf-0.1035  Tc 0.1035  Tw ( by K.) Tj29.25 0  TD 0  Tc 0  Tw ( ) Tj-407.25 -13.5  TD ( ) Tj3 0  TD ( ) Tj-3 -14.25  TD ( ) TjETendstreamendobj24 0 obj7752endobj22 0 obj<</Type /Page/Parent 5 0 R/Resources <</Font <</F1 8 0 R /F2 12 0 R /F4 25 0 R >>/ProcSet 2 0 R>>/Contents 23 0 R>>endobj6 0 obj<</Type /Font/Subtype /TrueType/Name /F0/BaseFont /TimesNewRoman,Bold/FirstChar 32/LastChar 255/Widths [ 250 333 555 500 500 1000 833 278 333 333 500 570 250 333 250 278 500 500 500 500 500 500 500 500 500 500 333 333 570 570 570 500 930 722 667 722 722 667 611 778 778 389 500 778 667 944 722 778 611 778 722 556 667 722 722 1000 722 722 667 333 278 333 581 500 333 500 556 444 556 444 333 500 556 278 333 556 278 833 556 500 556 556 444 389 333 556 500 722 500 500 444 394 220 394 520 778 500 778 333 500 500 1000 500 500 333 1000 556 333 1000 778 667 778 778 333 333 500 500 350 500 1000 333 1000 389 333 722 778 444 722 250 333 500 500 500 500 220 500 333 747 300 500 570 333 747 500 400 549 300 300 333 576 540 250 333 300 330 500 750 750 750 500 722 722 722 722 722 722 1000 722 667 667 667 667 389 389 389 389 722 722 778 778 778 778 778 570 778 722 722 722 722 722 611 556 500 500 500 500 500 500 722 444 444 444 444 444 278 278 278 278 500 556 500 500 500 500 500 549 500 556 556 556 556 500 556 500 ]/Encoding /WinAnsiEncoding/FontDescriptor 7 0 R>>endobj7 0 obj<</Type /FontDescriptor/FontName /TimesNewRoman,Bold/Flags 16418/FontBBox [ -250 -216 1175 1000 ]/MissingWidth 326/StemV 136/StemH 136/ItalicAngle 0/CapHeight 891/XHeight 446/Ascent 891/Descent -216/Leading 149/MaxWidth 979/AvgWidth 427>>endobj8 0 obj<</Type /Font/Subtype /TrueType/Name /F1/BaseFont /TimesNewRoman/FirstChar 32/LastChar 255/Widths [ 250 333 408 500 500 833 778 180 333 333 500 564 250 333 250 278 500 500 500 500 500 500 500 500 500 500 278 278 564 564 564 444 921 722 667 667 722 611 556 722 722 333 389 722 611 889 722 722 556 722 667 556 611 722 722 944 722 722 611 333 278 333 469 500 333 444 500 444 500 444 333 500 500 278 278 500 278 778 500 500 500 500 333 389 278 500 500 722 500 500 444 480 200 480 541 778 500 778 333 500 444 1000 500 500 333 1000 556 333 889 778 611 778 778 333 333 444 444 350 500 1000 333 980 389 333 722 778 444 722 250 333 500 500 500 500 200 500 333 760 276 500 564 333 760 500 400 549 300 300 333 576 453 250 333 300 310 500 750 750 750 444 722 722 722 722 722 722 889 667 611 611 611 611 333 333 333 333 722 722 722 722 722 722 722 564 722 722 722 722 722 722 556 500 444 444 444 444 444 444 667 444 444 444 444 444 278 278 278 278 500 500 500 500 500 500 500 549 500 500 500 500 500 500 500 500 ]/Encoding /WinAnsiEncoding/FontDescriptor 9 0 R>>endobj9 0 obj<</Type /FontDescriptor/FontName /TimesNewRoman/Flags 34/FontBBox [ -250 -216 1165 1000 ]/MissingWidth 323/StemV 73/StemH 73/ItalicAngle 0/CapHeight 891/XHeight 446/Ascent 891/Descent -216/Leading 149/MaxWidth 971/AvgWidth 401>>endobj12 0 obj<</Type /Font/Subtype /TrueType/Name /F2/BaseFont /Symbol/FirstChar 30/LastChar 255/Widths [ 600 600 250 333 713 500 549 833 778 439 333 333 500 549 250 549 250 278 500 500 500 500 500 500 500 500 500 500 278 278 549 549 549 444 549 722 667 722 612 611 763 603 722 333 631 722 686 889 722 722 768 741 556 592 611 690 439 768 645 795 611 333 863 333 658 500 500 631 549 549 494 439 521 411 603 329 603 549 549 576 521 549 549 521 549 603 439 576 713 686 493 686 494 480 200 480 549 600 600 600 600 600 600 600 600 600 600 600 600 600 600 600 600 600 600 600 600 600 600 600 600 600 600 600 600 600 600 600 600 600 600 620 247 549 167 713 500 753 753 753 753 1042 987 603 987 603 400 549 411 549 549 713 494 460 549 549 549 549 1000 603 1000 658 823 686 795 987 768 768 823 768 768 713 713 713 713 713 713 713 768 713 790 790 890 823 549 250 713 603 603 1042 987 603 987 603 494 329 790 790 786 713 384 384 384 384 384 384 494 494 494 494 600 329 274 686 686 686 384 384 384 384 384 384 494 494 494 600 ]/FontDescriptor 13 0 R>>endobj13 0 obj<</Type /FontDescriptor/FontName /Symbol/Flags 6/FontBBox [ -250 -220 1255 1005 ]/MissingWidth 334/StemV 109/StemH 109/ItalicAngle 0/CapHeight 1005/XHeight 503/Ascent 1005/Descent -220/Leading 225/MaxWidth 1046/AvgWidth 600>>endobj17 0 obj<</Type /Font/Subtype /TrueType/Name /F3/BaseFont /Symbol,Bold/FirstChar 30/LastChar 255/Widths [ 600 600 250 333 713 500 549 833 778 439 333 333 500 549 250 549 250 278 500 500 500 500 500 500 500 500 500 500 278 278 549 549 549 444 549 722 667 722 612 611 763 603 722 333 631 722 686 889 722 722 768 741 556 592 611 690 439 768 645 795 611 333 863 333 658 500 500 631 549 549 494 439 521 411 603 329 603 549 549 576 521 549 549 521 549 603 439 576 713 686 493 686 494 480 200 480 549 600 600 600 600 600 600 600 600 600 600 600 600 600 600 600 600 600 600 600 600 600 600 600 600 600 600 600 600 600 600 600 600 600 600 620 247 549 167 713 500 753 753 753 753 1042 987 603 987 603 400 549 411 549 549 713 494 460 549 549 549 549 1000 603 1000 658 823 686 795 987 768 768 823 768 768 713 713 713 713 713 713 713 768 713 790 790 890 823 549 250 713 603 603 1042 987 603 987 603 494 329 790 790 786 713 384 384 384 384 384 384 494 494 494 494 600 329 274 686 686 686 384 384 384 384 384 384 494 494 494 600 ]/FontDescriptor 18 0 R>>endobj18 0 obj<</Type /FontDescriptor/FontName /Symbol,Bold/Flags 16390/FontBBox [ -250 -220 1255 1005 ]/MissingWidth 334/StemV 191/StemH 191/ItalicAngle 0/CapHeight 1005/XHeight 503/Ascent 1005/Descent -220/Leading 225/MaxWidth 1046/AvgWidth 600>>endobj25 0 obj<</Type /Font/Subtype /TrueType/Name /F4/BaseFont /Arial/FirstChar 32/LastChar 255/Widths [ 278 278 355 556 556 889 667 191 333 333 389 584 278 333 278 278 556 556 556 556 556 556 556 556 556 556 278 278 584 584 584 556 1015 667 667 722 722 667 611 778 722 278 500 667 556 833 722 778 667 778 722 667 611 722 667 944 667 667 611 278 278 278 469 556 333 556 556 500 556 556 278 556 556 222 222 500 222 833 556 556 556 556 333 500 278 556 500 722 500 500 500 334 260 334 584 750 556 750 222 556 333 1000 556 556 333 1000 667 333 1000 750 611 750 750 222 222 333 333 350 556 1000 333 1000 500 333 944 750 500 667 278 333 556 556 556 556 260 556 333 737 370 556 584 333 737 552 400 549 333 333 333 576 537 278 333 333 365 556 834 834 834 611 667 667 667 667 667 667 1000 722 667 667 667 667 278 278 278 278 722 722 778 778 778 778 778 584 778 722 722 722 722 667 667 611 556 556 556 556 556 556 889 500 556 556 556 556 278 278 278 278 556 556 556 556 556 556 556 549 611 556 556 556 556 500 556 500 ]/Encoding /WinAnsiEncoding/FontDescriptor 26 0 R>>endobj26 0 obj<</Type /FontDescriptor/FontName /Arial/Flags 32/FontBBox [ -250 -212 1208 1000 ]/MissingWidth 276/StemV 80/StemH 80/ItalicAngle 0/CapHeight 905/XHeight 453/Ascent 905/Descent -212/Leading 150/MaxWidth 1007/AvgWidth 441>>endobj2 0 obj[ /PDF /Text  ]endobj5 0 obj<</Kids [4 0 R 14 0 R 19 0 R 22 0 R ]/Count 4/Type /Pages/MediaBox [ 0 0 612 792 ]>>endobj1 0 obj<</Creator <FEFF004B00650072006E0065006C00520069006400670065002E0064006F00630020002D0020004D006900630072006F0073006F0066007400200057006F00720064>/CreationDate (D:20051109161903)/Title <FEFF004B00650072006E0065006C00520069006400670065002E005000440046>/Author <FEFF00490073006100620065006C006C00650020004700750079006F006E>/Producer (Acrobat PDFWriter 4.05 for Windows NT)>>endobj3 0 obj<</Pages 5 0 R/Type /Catalog>>endobjxref0 270000000000 65535 f 0000047781 00000 n 0000047645 00000 n 0000048174 00000 n 0000009314 00000 n 0000047676 00000 n 0000040924 00000 n 0000042023 00000 n 0000042293 00000 n 0000043382 00000 n 0000000019 00000 n 0000009293 00000 n 0000043642 00000 n 0000044708 00000 n 0000022862 00000 n 0000009456 00000 n 0000022840 00000 n 0000044966 00000 n 0000046037 00000 n 0000032809 00000 n 0000023017 00000 n 0000032788 00000 n 0000040780 00000 n 0000032952 00000 n 0000040759 00000 n 0000046304 00000 n 0000047391 00000 n trailer<</Size 27/Root 3 0 R/Info 1 0 R/ID [<1a8773f627158e81217e960a6f7fb13d><1a8773f627158e81217e960a6f7fb13d>]>>startxref48223%%EOF