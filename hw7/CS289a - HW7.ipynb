{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 K-means Clustering\n",
    "\n",
    "Run k-means on MNIST with k = 5, 10, 20 cluster centers and visualize the centers. You must implement the k-means algorithm yourself.\n",
    "\n",
    "### 1.1. Use the image data at `mnist_data=images.mat`. There 60,000 unlabeled images. Each image contains 28 x 28 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from KMeans import KMeans\n",
    "from KNN import KNN\n",
    "from ALSLatentFactorModel import ALSLatentFactorModel\n",
    "from utils import plot_kmeans, kmeans_cost, validation_accuracy, \\\n",
    "    reshape_array, reshape_long_to_wide, wide_to_long\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(289)\n",
    "\n",
    "images = loadmat('./mnist_data/images.mat')['images'].T.reshape(60000, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k5 = KMeans(k=5, iterations=50)\n",
    "k5.fit(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k10 = KMeans(k=10, iterations=50)\n",
    "k10.fit(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k15 = KMeans(k=15, iterations=50)\n",
    "k15.fit(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Visualize the cluster centers, viewing each coordinate as a pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_kmeans(k5.kmeans, ncols=5, nrows=1, figsize=(15,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_kmeans(k10.kmeans, ncols=5, nrows=2, figsize=(15,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_kmeans(k15.kmeans, ncols=5, nrows=3, figsize=(15,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Repeat the algorithm for multiple times with random initialization. Does the k-mean loss vary in different runs?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Yes, k-means loss does vary over repeated runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmeans5_1 = KMeans(k=5, iterations=50)\n",
    "k5_1.fit(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmeans5_2 = KMeans(k=5, iterations=50)\n",
    "kmeans5_2.fit(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmeans5_3 = KMeans(k=5, iterations=50)\n",
    "kmeans5_3.fit(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[kmeans_cost(x) for x in [kmeans5, kmeans5_1, kmeans5_2, kmeans5_3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Joke Recommender System\n",
    "\n",
    "You will build a personalized joke recommender system. There are $m = 100$ jokes and $n = 24,983$ users. As historical data, every user read a subset of jokes and rated them. The goal is to recommend more jokes, such that the recommended jokes match the individual user's sense of humour.\n",
    "\n",
    "### 2.1 Data Format\n",
    "The historical rating is represented by a matrix $R \\in R^{n \\times m}$. The entry $R_{ij}$ represents the user $i$'s rating on joke $j$. The rating is a real number in [-10, 10]: a higher value represents that the user is more satisfied with the joke. If the joke is not rated, then the corresponding entry value is `NaN`.\n",
    "\n",
    "The directory `joke_data/jokes/` contains the text of all 100 jokes. Read them before you start! In addition, you are provided with three files at `joke data/`:\n",
    "\n",
    "* `joketrain.mat` is given as the training data. It contains the matrix R specified above.\n",
    "* `validation.txt` contains user-joke pairs that doesn't appear in the training set. Each line takes the form `\"i,j,s\"`, where `i` is the user index, `j` is the joke index, `s` indicates whether the user likes the joke. More specifucally, `s = 1` if and only if the user gives a positive rating to the joke.\n",
    "* `query.txt` contains user-joke pairs that are neither in the training set nor in the validation set. Each line takes the form `\"id,i,j\"`. You are asked to predict if user `i` likes joke `j`. The integer id is a unique id for the user-joke pair. Use it to submit the prediction to Kaggle (see Section 2.4).\n",
    "\n",
    "### 2.2 Warm-up\n",
    "Let's start with the simplest recommender system: recommend the joke by its average rating in the training set. In particular, assert that any user will like the joke if its average rating is positive. Report your prediction accuracy on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jokes_nan = loadmat('./joke_data/joke_train.mat')['train']\n",
    "validation = np.loadtxt('./joke_data/validation.txt', delimiter=',')\n",
    "query = np.loadtxt('./joke_data/query.txt', delimiter=',', dtype=np.int)\n",
    "\n",
    "joke_avg_score_array = np.where(np.nanmean(jokes_nan, axis=0) > 0, 1., 0.)\n",
    "joke_avg_scores = np.array([joke_avg_score_array,]*100)\n",
    "validation_accuracy(validation, joke_avg_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we turn to a more advanced system where personal preferences are taken into account. Replace all missing values of $R$ by zero, then measure the distance of any two users by the Euclidean distance between their rating vectors. For each user, find the k nearest neighbors of him/her, then make the prediction by averaging the ratings of these neighbors. Report your prediction accuracy on the validation set with $k = 10, k = 100$ and $k = 1,000$. How are the accuracies compare to the simple system?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "KNN with k = 100 and 1000 did better than simply using the average for all ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jokes_knn = jokes_nan.copy()\n",
    "jokes_knn[np.isnan(jokes_knn)] = 0\n",
    "\n",
    "user_ids = np.unique(validation[:,0])\n",
    "jokes_test = reshape_long_to_wide(validation, user_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "knn_10 = KNN(k=10)\n",
    "knn_10.fit(jokes_knn)\n",
    "jokes_knn10_scores = knn_10.predict(jokes_test)\n",
    "validation_accuracy(validation, jokes_knn10_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "knn_100 = KNN(k=100)\n",
    "knn_100.fit(jokes_knn)\n",
    "jokes_knn100_scores = knn_100.predict(jokes_test)\n",
    "validation_accuracy(validation, jokes_knn100_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "knn_1000 = KNN(k=1000)\n",
    "knn_1000.fit(jokes_knn)\n",
    "jokes_knn1000_scores = knn_1000.predict(jokes_test)\n",
    "validation_accuracy(validation, jokes_knn1000_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Latent Factor Model\n",
    "\n",
    "Latent factor model is the state-of-the-art method for personalized recommendation. It learns a vector representation $u_i \\in \\mathbb{R}^d$ for each user and a vector representation $v_j \\in \\mathbb{R}^d$ for each joke, such that the inner product $\\langle u_i, v_j \\rangle$ approximates the rating $R_{ij}$ . You will build a simple latent factor model.\n",
    "\n",
    "#### 2.3.1 Replace all missing values by zero. Then use principle component analysis (PCA) to learn vector representation for users and jokes. Note that you are NOT allowed to use any inbuilt functions for PCA in MATLAB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jokes_centered = jokes_knn - jokes_knn.mean(axis=0)\n",
    "\n",
    "U, s, Vt = np.linalg.svd(jokes_centered, full_matrices=False)\n",
    "V = Vt.T\n",
    "\n",
    "# sort the PCs by descending order of the singular values (i.e. by the proportion of total variance they explain)\n",
    "ind = np.argsort(s)[::-1]\n",
    "U = U[:, ind]\n",
    "s = s[ind]\n",
    "V = V[:, ind]\n",
    "S = np.diag(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Evaluate the learnt vector representations by mean squared error:\n",
    "\n",
    "$$\\textrm{MSE} = \\sum_{(i,j) \\in S} (\\langle u_i, v_j \\rangle - R_{ij})^2, \\textrm{where} \\; S := \\{(i,j) : R_{ij} \\neq NaN\\}$$\n",
    "\n",
    "#### Try $d = 2, 5, 10, 20$. How does the MSE vary as a function of $d$? Use the inner product $\\langle u_i, v_i\\rangle$ to predict if user $i$ likes joke $j$. Report prediction accuracies on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for d in [2, 5, 10, 20]:\n",
    "    SVt  = S[:d,:d].dot(V[:,:d].T)\n",
    "    USVt = U[:,:d].dot(SVt)\n",
    "    prediction = np.where(USVt > 0, 1., 0.)\n",
    "    print(\"D = {}\".format(d))\n",
    "    print(\"* MSE = {:.2f}\".format(np.mean((USVt - jokes_centered)**2)))\n",
    "    print(\"* Accuracy = {:.2f}\".format(validation_accuracy(validation, prediction)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3  For sparse data, replacing all missing values by zero is not a completely satisfying solution. A missing value means that the user has not read the joke, but doesn't mean that the rating should be zero. A more reasonable choice is to minimize the MSE only on rated joke. Let's define a loss function:\n",
    "\n",
    "$$L \\Big(  \\{ u_i \\}, \\{ v_i \\} \\Big) := \\sum_{(i,j) \\in S} (\\langle u_i, v_j \\rangle - R_{ij})^2 + \\lambda \\sum^n_{i=1} \\| u_i \\|^2_2 + \\lambda \\sum_{j=1}^m \\| v_j \\|^2_2$$\n",
    "\n",
    "#### where set S has the same definition as in equation (1) and $\\lambda > 0$ is the regularization coefficient. Implement an algorithm to learn vector representations by minimizing the loss function $L(\\{u_i\\},\\{v_j\\})$.\n",
    "\n",
    "**Hint:** you may want to employ an alternating minimization scheme. First, randomly initialize $\\{u_i\\}$ and $\\{v_j\\}$. Then minimize the loss function with respective to $\\{u_i\\}$ by treating $\\{v_j\\}$ as constant vectors, and minimize the loss function with respect to $\\{v_j\\}$ by treating $\\{u_i\\}$ as constant vectors. Iterate these two steps until both $\\{u_i\\}$ and $\\{v_j\\}$ converge. Note that when one of $\\{u_i\\}$ or $\\{v_j\\}$ is given, minimizing the loss function with respect to the other part has closed-form solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.4 Compare the resulting MSE and the prediction error with Step 2. Note that you need to tune the hyper-parameter $\\lambda$ to optimize the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "als = ALSLatentFactorModel(d=10, λ=.01, num_iters=10)\n",
    "als.fit(jokes_nan)\n",
    "als_predictions = als.predict()\n",
    "validation_accuracy(validation, als_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Recommending Jokes\n",
    "\n",
    "#### 2.4.1 Using the methods you have implemented to predict the users' preference on unread jokes. For each line `\"id,i,j\"` in the query file, output a line `\"id,s\"` to a file named kaggle_submission.txt. Here, `s = 1` means that user `i` will give a positive rating to joke `j`, while `s = 0` means that the user will give a non-positive rating. The first line of `kaggle_submission.txt` should be the field titles: `\"Id,Category\"` . Submit `kaggle_submission.txt` to Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kaggle_array = wide_to_long(als_predictions, query)\n",
    "kaggle_submission = np.vstack((query[:,0], kaggle_array)).T\n",
    "np.savetxt(\"./als_d5_kaggle.csv\", kaggle_submission, fmt=\"%i\", delimiter=\",\", header=\"Id,Category\", comments=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
