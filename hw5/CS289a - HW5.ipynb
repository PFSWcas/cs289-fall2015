{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. You must implement Decision Trees and Random Forests, then train each on the spam data, and use each to separately classify the spam data. Repeat for the census income classification. See the report below.\n",
    "\n",
    "### 2. You are not allowed to use any off the shelf decision tree/random forest implementation for the homework. You can use external libraries for data preprocessing (in fact, we recommend it). You can use any programming language you wish to as long as we can read and run your code with minimal effort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Code and report requirements:\n",
    "\n",
    "> ### 3.a) For spam, if you use any other features or feature transformations, explain what you did in your report. You may choose to use something like bag-of- words. You should implement any custom feature extraction code in `featurize.py`, which will save your features to a `.mat` file.\n",
    "\n",
    "> **Answer:**\n",
    "\n",
    "> No other additional features/transformations were used in my code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 3.b) A report of the results you obtained. List the performance of your decision tree and random forest on your validation set. Also list your single best Kaggle score with any method (state which method you used)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:**\n",
    "\n",
    "> My best score on Kaggle was 75.7% accuracy using Decision Tree with a maximum tree depth of 10 and at least 5 observations per node. However, while tuning my models and submitting results that demonstrated improved cross-validation accuracy, I kept getting the same score on Kaggle. I only realized after I had exceeded the maximum number of allowable submissions that I was actually submitting an earlier version of my predictions rather than the newer, better versions. This was due to my a global variable that contained older results that was being converted into a CSV file for submission. Alas...\n",
    "\n",
    "> My performance against a small hold out set using Decision Trees was 80.2%. I used a maximum tree depth of 12, and at least 5 observations per node.\n",
    "\n",
    "> My performance against a small hold out set using Random Forests was 81.5%. My hyperparameters included a total of 20, maximum depth of 10, at least 5 observations per node, and 50 trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from DecisionTree import *\n",
    "from RandomForest import *\n",
    "from scipy.io import loadmat\n",
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spam = loadmat('./spam-dataset/spam_data.mat')\n",
    "spam_test = spam['test_data']\n",
    "spam_train = spam['training_data']\n",
    "spam_train_n = spam_train.shape[0]\n",
    "spam_train_label = spam['training_labels'][0]\n",
    "first_obs = spam_train[0,:].reshape(1,32)\n",
    "\n",
    "# # shuffle data\n",
    "np.random.seed(289)\n",
    "spam_train_combined = np.hstack((spam_train, spam_train_label.reshape(spam_train_n,1)))\n",
    "np.random.shuffle(spam_train_combined)\n",
    "\n",
    "spam_train_label = spam_train_combined[:,32]\n",
    "spam_train = spam_train_combined[:,:32]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spam Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80208333333333337"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_spam = DecisionTree(max_depth=12, min_obs=5)\n",
    "dt_spam.fit(spam_train[:4500,:], spam_train_label[:4500])\n",
    "dt_spam_pred = dt_spam.predict(spam_train[4500:,:])\n",
    "sum(dt_spam_pred == spam_train_label[4500:])/len(spam_train_label[4500:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spam Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81547619047619047"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_spam = RandomForest(num_features=20, max_depth=10, min_obs=5, num_trees=50)\n",
    "rf_spam.fit(spam_train[:4500,:], spam_train_label[:4500])\n",
    "rf_spam_pred = rf_spam.predict(spam_train[4500:,:])\n",
    "sum(rf_spam_pred == spam_train_label[4500:])/len(spam_train_label[4500:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# kaggle_spam = np.vstack((np.arange(1,len(rf_spam_pred)+1), rf_spam_pred)).T\n",
    "# np.savetxt(fname=\"./spam_prediction_rf_f20_md10_mo5.csv\", X=kaggle_spam, fmt='%d', \n",
    "#            delimiter=',', header='Id,Category', comments='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 3.c) For your decision tree, and for a data point of your choosing, state what splits (i.e. which feature and which value of that feature to split on) that your decision tree made to classify it. An example of what this might look like:\n",
    "\n",
    "> i. (\"viagra\") >= 2\n",
    "\n",
    "> ii. (\"thanks\") < 1\n",
    "\n",
    "> iii. (\"nigeria\") >= 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:**\n",
    "\n",
    "> For the first observation in the training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         2.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  1.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> the splits were as follows:\n",
    "```\n",
    "! (feature 28) > 0.0\n",
    "pain (feature 0) <= 0.0\n",
    "& (feature 31) <= 0.0\n",
    "money (feature 3) <= 0.0\n",
    "$ (feature 26) <= 1.0\n",
    "( (feature 29) <= 0.0\n",
    "message (feature 15) <= 0.0\n",
    "# (feature 27) <= 1.0\n",
    "business (feature 14) <= 1.0\n",
    "other (feature 12) <= 1.0\n",
    "! (feature 28) <= 2.0\n",
    "$ (feature 26) <= 0.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 3.d) For random forests, find and state the most common splits made at the root node of the trees. For example:\n",
    "\n",
    "> i. (\"viagra\") >= 3 (20 trees)\n",
    "\n",
    "> ii. (\"thanks\") < 4 (15 trees)\n",
    "\n",
    "> iii. (\"nigeria\") >= 1 (5 trees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:**\n",
    "\n",
    "> For the first data point in the training set, the following splits were made:\n",
    "```\n",
    "pain (feature 0) <= 0.0 (11 trees)\n",
    "featured (feature 9) > 0.0 (3 trees)\n",
    "featured (feature 9) <= 0.0 (2 trees)\n",
    "spam (feature 5) <= 0.0 (2 trees)\n",
    "bank (feature 2) > 0.0 (2 trees)\n",
    "bank (feature 2) <= 0.0 (2 trees)\n",
    "path (feature 18) <= 0.0 (2 trees)\n",
    "revision (feature 17) > 0.0 (2 trees)\n",
    "volumes (feature 16) > 0.0 (2 trees)\n",
    "volumes (feature 16) <= 0.0 (2 trees)\n",
    "business (feature 14) > 0.0 (2 trees)\n",
    "energy (feature 13) > 0.0 (2 trees)\n",
    "private (feature 1) > 0.0 (2 trees)\n",
    "height (feature 8) > 0.0 (1 tree)\n",
    "creative (feature 7) <= 0.0 (1 tree)\n",
    "spam (feature 5) > 0.0 (1 tree)\n",
    "drug (feature 4) <= 0.0 (1 tree)\n",
    "money (feature 3) <= 0.0 (1 tree)\n",
    "meter (feature 19) > 0.0 (1 tree)\n",
    "meter (feature 19) <= 0.0 (1 tree)\n",
    "path (feature 18) > 0.0 (1 tree)\n",
    "revision (feature 17) <= 0.0 (1 tree)\n",
    "width (feature 11) > 0.0 (1 tree)\n",
    "width (feature 11) <= 0.0 (1 tree)\n",
    "differ (feature 10) > 0.0 (1 tree)\n",
    "differ (feature 10) <= 0.0 (1 tree)\n",
    "pain (feature 0) > 0.0 (1 tree)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. (a), (b), (c), (d). Repeat the same parts for the census dataset. You will need to do your own feature processing. In part (a), you must report what you did to handle categorical variables and missing features. See the Appendix for detailed instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> My best score on Kaggle was 85.4% accuracy. I used a Decision Tree with maximum depth of 15 and at least 20 observations per node.\n",
    "\n",
    "> My performance against a small hold out set using Decision Trees (with the same hyperparameters as my Kaggle submisison) was 85.37%.\n",
    "\n",
    "> My performance against a small hold out set using Random Forests was 85.1%. My hyperparameters included a total of 50 features, maximum depth of 10, at least 15 observations per node, and 20 trees. I had previously used a 100 trees in my Random Forest but training time took too long.\n",
    "\n",
    "> I used the DictVectorizer function to perform binary one-hot coding of categorical data and treated the missing data as a separate categorical rather than performing imputation. The `fnlwgt` variable was also converted into a categorical variable from a numeric variable. I binned these values into one of 10 categories based on the range of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "census_train = pd.read_csv('./census-dataset/data.csv')\n",
    "census_train_label = census_train.label\n",
    "census_train = census_train.drop('label', 1)\n",
    "census_test = pd.read_csv('./census-dataset/test_data.csv')\n",
    "\n",
    "# feature engineering: convert numeric to categorical\n",
    "fnlwgt_bins = np.linspace(0, 1500000, 10)\n",
    "census_train.fnlwgt = pd.cut(census_train.fnlwgt, fnlwgt_bins)\n",
    "census_test.fnlwgt = pd.cut(census_test.fnlwgt, fnlwgt_bins)\n",
    "\n",
    "# feature engineering: perform hot-one coding\n",
    "dv = DictVectorizer(sparse=False)\n",
    "census_dict_train = census_train.T.to_dict().values()\n",
    "census_dict_test = census_test.T.to_dict().values()\n",
    "census_train = dv.fit_transform(census_dict_train)\n",
    "census_test = dv.transform(census_dict_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85370274469186946"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = DecisionTree(max_depth=15, min_obs=20)\n",
    "dt.fit(census_train[:25000,:], census_train_label[:25000])\n",
    "dt_census_pred = dt.predict(census_train[25000:,:])\n",
    "sum(dt_census_pred == census_train_label[25000:])/len(census_train_label[25000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85111341273951324"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForest(num_features=50, max_depth=10, min_obs=15, num_trees=20)\n",
    "rf.fit(census_train[:25000,:], census_train_label[:25000])\n",
    "rf_census_pred = rf.predict(census_train[25000:,:])\n",
    "sum(rf_census_pred == census_train_label[25000:])/len(census_train_label[25000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# kaggle_census = np.vstack((np.arange(1,len(rf_census_pred)+1), rf_census_pred)).T\n",
    "# np.savetxt(fname=\"./census_predictions_tree_md15_mo20.csv\", X=kaggle_census, fmt='%d', \n",
    "#            delimiter=',', header='Id,Category', comments='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.a) An explanation of the decision tree techniques you implemented (stopping criteria, pruning, dealing with missing attributes, splitting criteria other than entropy, heuristics for faster training, complex decisions at nodes, cross-validation, Adaboost, bagging etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "> **Answer:**\n",
    "\n",
    "> I used several techniques for my decision trees. I used a maximum depth and minimum number of observations per node as stopping criteria. I did not do any pruning. I resolved issues involving missing attributes by using the DictVectorizer to automatically convert them into categorical features rather than perform imputation. I also converted the numerical feature `fnlwght` into a categorical feature by binning the values into 10 categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.b) An explanation of the random forest techniques you used. If the decision trees you used inside your random forest were di\u000b",
    "erent than your standalone decision tree implementation, explain how."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "> **Answer:**\n",
    "\n",
    "> In addition to the criteria I used for decision trees, I specified the number of features and trees to use for training. I bootstraped data and also let each tree see only a subset of the features. After building the individual trees, I averaged their results for classification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
