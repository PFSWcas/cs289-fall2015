{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Linear Regression\n",
    "\n",
    "In this problem we will try to predict the median home value in a given Census area by using linear regression. The data is in `housing_data.mat`, and it comes from http://lib.stat.cmu.edu/datasets/ (`houses.zip`). There are only 8 features for each data point; you can read about the features in `housing_data_source.txt`.\n",
    "\n",
    "1.) Implement a linear regression model with least squares. Include your code in the submission. You should add a constant term to the training data (e.g. add another dimension to each data point, with the value of 1). This is same as adding the bias term to linear regression (see discussion 4 question 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "housing  = scipy.io.loadmat('data/housing_data.mat')\n",
    "h_Xtrain = np.asmatrix(housing['Xtrain'])\n",
    "h_Ytrain = np.asmatrix(housing['Ytrain'])\n",
    "h_Xtest  = np.asmatrix(housing['Xvalidate'])\n",
    "h_Ytest  = np.asmatrix(housing['Yvalidate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_ones(X):\n",
    "    m = X.shape[0]\n",
    "    ones = np.ones((m, 1))\n",
    "    X1 = np.hstack((ones, X))\n",
    "    return X1\n",
    "\n",
    "def least_squares(X, y):\n",
    "    X1 = add_ones(X)\n",
    "    X1_T = X1.T\n",
    "    w = np.linalg.inv(X1_T*X1)*X1_T*y\n",
    "    return w\n",
    "\n",
    "def rss_calc(y, y_hat):\n",
    "    rss = np.sum(np.square(y-y_hat))\n",
    "    return rss\n",
    "\n",
    "def predict(X, w):\n",
    "    X1 = add_ones(X)\n",
    "    y_hat = X1*w\n",
    "    return y_hat\n",
    "\n",
    "w = least_squares(h_Xtrain, h_Ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.) Test your trained model on the validation set. What is the residual sum of squares (RSS) on the validation set? What is the range of predicted values? Do they make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5794953797672.457"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_Ytest_hat = predict(h_Xtest, w)\n",
    "rss_calc(h_Ytest, h_Ytest_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.) Plot the regression coefficient $w$ (plot the value of each coefficient against the index of the coefficient). Be sure to exclude the coefficient corresponding to the constant offset you added earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": [
       "iVBORw0KGgoAAAANSUhEUgAAAYkAAAD7CAYAAACfQGjDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\n",
       "AAALEgAACxIB0t1+/AAAGWtJREFUeJzt3XuwXGWd7vHvYyIICGTQOdwSICUBCXgBpsDjNcrFSDlA\n",
       "1TgSqlRmJuecGRmF0ZGRgAM4XsHjYfA4iA6ogIo3RsGC4qZkPOoBBEHuEBgzkiBBgoSLIgSe88da\n",
       "2zT77N7Zvbt3v726n09V1179rrW6nw7av/6tt3st2SYiImIizysdICIiBleKREREtJUiERERbaVI\n",
       "REREWykSERHRVopERES0Nbt0gF6RlO/yRkR0yLYmWz80RQI2/mIHlaRTbJ9SOsd0JX9ZyV9Wk/NP\n",
       "5cN1DjdFRERbKRIREdFWisRgWF46QJeWlw7QpeWlA3RpeekAXVpeOkCXlpcOMJM0LOdukuSmzklE\n",
       "RJQwlffNdBIREdFWikRERLSVIhEREW2lSERERFspEhER0VaKREREtJUiERERbaVIREREWykSERHR\n",
       "VopERES0lSIRERFtpUhERERbPSkSkmZJulHS9+r720i6UtLdkq6QNKdl22WSVki6U9LBLeP7Srql\n",
       "XndGy/imkr5Rj18jaedeZI6IiI3rVSdxLHA7MHZK2eOBK23vBny/vo+khcARwEJgMXCmpLEzEH4O\n",
       "WGp7AbBA0uJ6fCmwth4/HTi1R5kjImIjui4SkuYChwBnA2Nv+IcC59bL5wKH18uHARfYftr2SuAe\n",
       "YH9J2wNb2r6u3u68ln1aH+tC4IBuM0dExNT0opM4HTgOeLZlbFvba+rlNcC29fIOwKqW7VYBO04w\n",
       "vroep/57H4Dt9cA6Sdv0IHdERGzE7G52lvRW4EHbN0paNNE2tj2Vi233grTn5+H2X9V3l9te3o/n\n",
       "jYhogvp9elEn+3RVJIBXA4dKOgR4AbCVpPOBNZK2s/1AfSjpwXr71cC8lv3nUnUQq+vl8eNj++wE\n",
       "3C9pNrC17YcnjnPbS4C/sRmOy+1FRPRQ/cF5+dh9SSdvbJ+uDjfZPsH2PNvzgSXAD2y/E7gYOKre\n",
       "7Cjgu/XyxcASSZtImg8sAK6z/QDwqKT964nsdwIXtewz9lhvo5oIb2cecPAk6yMiogPddhLjjX2C\n",
       "/yTwTUlLgZXA2wFs3y7pm1TfhFoPHO0NF9k+GvgysBlwqe3L6vFzgPMlrQDWUhWjdpYBp0pcZfNM\n",
       "z15VRMSI0ob36Gar5j38PODHwFk255XOFBExyCTZtibdZpiKRDVHzmuBrwK72zxZOldExKCaSpEY\n",
       "utNy2PwIuBF4T+ksERFNN3SdRLXMHsAPqbqJNt+EiogYbSPZSQDY3AF8h2oiOyIipmkoO4nqPjsA\n",
       "twB72/yyXLKIiME0sp0EgM39wJnAR0pniYhoqqHtJKoxtgLuBt5s8/MyySIiBtNIdxIANo8CHyOn\n",
       "F4+ImJahLhK1zwO7SjnFeEREp4a+SNg8BZwAnCYN/+uNiOilUXnT/BbwDNVV8SIiYoqGeuL6uetZ\n",
       "BHwJeKnN7/sWLCJiQI38xHUrm+XAbcC7C0eJiGiMkekkqm3Yi+p6FLvZrOtPsoiIwZROYhybW4FL\n",
       "gA+WzhIR0QQj1UlU2zEX+DnwCvsPl0iNiBg56SQmUBeGLwAfLp0lImLQjVwnUW3LHKrTdbypPgQV\n",
       "ETFy0km0YfMI8Amqa3FHREQbI1kkamcCe0q8oXSQiIhBNbJFov5B3YlUp+uY0mGqiIhRM7JFovZ1\n",
       "YDbwttJBIiIG0UhOXD93Pw4EzgIW1icDjIgYCZm4ngKbq4B7gf9ROktExKAZ+U6i2pdXApdRna7j\n",
       "0d4mi4gYTOkkpsjmJuAK4AOls0REDJJ0En/Yn52BnwF72fyqd8kiIgbTlE6MmiLR+hh8CtjK5q97\n",
       "FCsiYmClSHT8GGwD3AW8zubO3iSLiBhMmZPokM3DwGlUp+yIiBh56ST+v8fhBVQn/zvS5sfdJ4uI\n",
       "GEzpJKbB5kngQ8CncrqOiBh1KRIT+yqwBXB46SARESWlSEzA5hmqS5x+UuL5pfNERJSSItHe5cAq\n",
       "YGnpIBERpXRVJCTNk3S1pNsk3SrpmHp8G0lXSrpb0hWS5rTss0zSCkl3Sjq4ZXxfSbfU685oGd9U\n",
       "0jfq8Wsk7dxN5qmyMfAPwEkSL+zHc0ZEDJpuO4mngffZ3hN4FfC3kvYAjgeutL0b8P36PpIWAkcA\n",
       "C4HFwJmSxiaHPwcstb0AWCBpcT2+FFhbj58OnNpl5imzuQFYDry/X88ZETFIuioSth+wfVO9/Dhw\n",
       "B7AjcChwbr3ZuWyYAD4MuMD207ZXAvcA+0vaHtjS9nX1due17NP6WBcCB3STeRpOBI6V2LbPzxsR\n",
       "UVzP5iQk7QLsDVwLbGt7Tb1qDfzhDXYHquP8Y1ZRFZXx46vrceq/9wHYXg+sk7RNr3JvjM0vgPOB\n",
       "k/r1nBERg6InRULSC6k+5R9r+7HWda5+rdf0X+x9FDhCYkHpIBER/TS72weQ9HyqAnG+7e/Ww2sk\n",
       "bWf7gfpQ0oP1+GpgXsvuc6k6iNX18vjxsX12Au6XNBvY2vbDbbKc0nJ3ue3l035hLWwekvg08HHg\n",
       "z3vxmBER/SZpEbCoo326OS1HPel8LtXE8vtaxk+rx06VdDwwx/bx9cT114D9qA4jXQXsatuSrgWO\n",
       "Aa4DLgE+Y/sySUcDL7P9bklLgMNtL5kgS09Oy9H+tbI51ek63mZzzUw9T0REv8z4WWAlvRb4IXAz\n",
       "Gw4pLaN6o/8mVQewEni77UfqfU4A/gpYT3V46vJ6fF/gy8BmwKW2x75OuynVnMDewFpgST3p3fGL\n",
       "7ZbEUuAo4A31V2QjIhorpwrv+XMwG7gJWGbzvZl8roiImZYT/PWYzXqq33x8si4YERFDLUWic5cA\n",
       "DwF/UThHRMSMy+GmaT0X+1N9o2s3m9/24zkjInoth5tmiM21wE+AvyudJSJiJqWTmPbzsStwDbCH\n",
       "za/79bwREb2SbzfN+HPyWeAZm2P7+bwREb2QIjHjz8l/AW4H9rP5j34+d0REtzInMcNsHgTOAD5W\n",
       "OktExExIJ9H187IFsAI41Ob6fj9/RMR0pZPoA5sngA8Dp0n0vUhFRMykFIneOIfqmhiLN7ZhREST\n",
       "pEj0QMvpOk6VmFU6T0REr6RI9M5FwGPAO0oHiYjolUxc9zQDrwa+Duxu87uSWSIiNiYT131m8xPg\n",
       "euC9pbNERPRCOome52B34EdU3cSEl1mNiBgE6SQKsLmL6gyxJ5TOEhHRrXQSM0Bie+BWYF+blYXj\n",
       "RERMKJ1EITa/Aj4LfKR0loiIbqSTmCESWwJ3A4fY3Fg6T0TEeOkkCrJ5DPgocGrpLBER05UiMbO+\n",
       "AMyXOKh0kIiI6UiRmEE2TwPLqE7XkX/riGicvHHNvAuBp4AjSweJiOhUJq77QOL1wHnAS22eLJ0n\n",
       "IgIycT0wbH4I3AIcXTpLREQn0kn0icSewNXAbjaPlM4TEZFOYoDY3AZcTHXdiYiIRkgn0UcSOwI3\n",
       "A6+0ua90nogYbekkBozNauAs4J9KZ4mImIp0En0msTXV6ToOtLmldJ6IGF3pJAaQzTrg48AnS2eJ\n",
       "iNiYFIkyzgL2kHhj6SAREZNJkSjA5vdUFyU6LafriIhBljeocr4JCPjz0kEiItrJxHVBEm8C/hXY\n",
       "w+ap0nkiYrQM1cS1pMWS7pS0QtIHS+fpBZsfUH3T6W9KZ4mImEgjOglJs4C7gAOB1cBPgSNt39Gy\n",
       "TeM6CQCJlwM/Bn4HrAMebfn7aAdjj9qs73f+iGiuqbxvzu5XmC7tB9xjeyWApK8DhwF3TLZTE9jc\n",
       "LPHHwBxgq/q29bi/WwF/DLxkkvVbSTxJZ4VlorHHbJ6d6dcdEc3QlCKxIzznNBargP0LZem5+vTh\n",
       "D9S3aZEQsAXPLRwTLe8I7DHJ+i0knqCzwvJbqkl4UR3CbNpfgGcnuLnNeIl1k60f+7enZXkmb718\n",
       "nqeBb9j8jhhITSkSUzomJumUlrvLbS+fkTQDyMbA4/Vt9XQfR2IW8ELaF5Gxv7u0LG/OhjewQfr7\n",
       "zBS3gw0FY/ytdXz2JOsm269X6yZaP6t+DWM3xt3v963T538FsBM5VU1fSFoELOpon4bMSbwKOMX2\n",
       "4vr+MuBZ26e2bNPIOYmIUSaxK3ANsMDmN6XzjJph+nbT9cACSbtI2gQ4guq02xHRYDb3ABcB7yud\n",
       "JSbWiE4CQNJbgH+maq/Psf2JcevTSUQ0kMR8qg+Cu9msLZ1nlEzlfbMxRWJjUiQimkviC8BDNieU\n",
       "zjJKUiQiohEkdgZuBHa3+XXpPKNimOYkImKI2fwn8HXguNJZ4rnSSUTEQJCYS3V53z1s1pTOMwrS\n",
       "SUREY9isAr4CDMW52YZFOomIGBgSOwC3Anva/Kp0nmGXTiIiGsXmfuDLwPGFo0QtnUREDBSJbalO\n",
       "3vny+hBUzJB0EhHROPWk9dnAstJZIp1ERAyg+vT5dwJ72/yydJ5hlU4iIhqp/kHd54ETS2cZdekk\n",
       "ImIgSbyI6vK+f2Lzi9J5hlE6iYhorPpkf/8CfKh0llGWTiIiBpbEHwErgFfVpxWPHkonERGNVl+I\n",
       "6DPAP5bOMqrSSUTEQJPYGrgHeK3NXaXzDJN0EhHReDbrqC44dlLpLKMonUREDDyJLYF7gUU2t5fO\n",
       "MyzSSUTEULB5DPg0cHLpLKMmnURENILEC6nmJg6yuaV0nmGQTiIihobN48CnSDfRV+kkIqIxJDan\n",
       "6iYOsbmpdJ6mSycREUPF5rfAqcAphaOMjHQSEdEoEptR/Qr7MJsbSudpsnQSETF0bH4HfAL4cOks\n",
       "oyBFIiKa6Gzg5RL7lw4y7FIkIqJxbH4PfIx0EzMuRSIimupLwEslXlM6yDBLkYiIRrJ5CvgI6SZm\n",
       "VIpERDTZecB8iTeUDjKsUiQiorFsnqbqJv5JIl+BnwEpEhHRdF8BdgDeWDrIMEqRiIhGs1lPNS+R\n",
       "bmIGpEhExDC4AHgxcFDpIMMmRSIiGs/mGarzOX043URvpUhExLD4FrAlsLh0kGEy7SIh6VOS7pD0\n",
       "c0n/JmnrlnXLJK2QdKekg1vG95V0S73ujJbxTSV9ox6/RtLOLeuOknR3fXvXdPNGxHBr6SYyN9FD\n",
       "3XQSVwB72n4FcDewDEDSQuAIYCFVRT9T0th/sM8BS20vABZIGqv4S4G19fjpVKcCRtI2VBc/36++\n",
       "nSxpTheZI2K4/RuwCfDW0kGGxbSLhO0rbT9b370WmFsvHwZcYPtp2yupLhCyv6TtgS1tX1dvdx5w\n",
       "eL18KHBuvXwhcEC9/GbgCtuP2H4EuJK0khHRhs2zVFeuSzfRI72ak/gr4NJ6eQdgVcu6VcCOE4yv\n",
       "rsep/94HYHs9sE7SiyZ5rIiIdi4CzIYPodGF2ZOtlHQlsN0Eq06w/b16mxOBp2x/bQbyRUR0xMYS\n",
       "JwEfl7io7i5imiYtErYn/c6xpL8ADmHD4SGoOoR5LffnUnUAq9lwSKp1fGyfnYD7Jc0Gtra9VtJq\n",
       "YFHLPvOAH0yS55SWu8ttL58sf0QMrUuo5jP/jOpbTwFIWsRz31M3vs90L19aTzp/GniD7YdaxhcC\n",
       "X6OaaN4RuArY1bYlXQscA1xH9R/xM7Yvk3Q08DLb75a0BDjc9pJ64vp6YB9AwA3APvX8xPg8uXxp\n",
       "RPyBxNh71Mvrbz7FOFN53+ymSKyg+hbBw/XQ/7V9dL3uBKp5ivXAsbYvr8f3Bb4MbAZcavuYenxT\n",
       "4Hxgb2AtsKSe9EbSXwIn1M/xUdtjE9wdv9iIGB31xPWPgf9tc0HpPINoRovEoEmRiIjxJA4CPgvs\n",
       "WZ/jKVpM5X0zv7iOiGF2FfAgcGTpIE2VTiIihprEG4EvAHukm3iudBIRMfJsrqb6JuU7SmdponQS\n",
       "ETH0JF5P9aWZ3eur2QXpJCIiALD5IXAvcFTpLE2TTiIiRoLEq6l+w7WbzVOl8wyCdBIRETWbnwB3\n",
       "UP2GK6YonUREjAyJ/ajONL3A5snSeUpLJxER0cLmOuAm4L+VztIU6SQiYqRI7AN8D9jV5nel85SU\n",
       "TiIiYhybn1GdZPSvS2dpgnQSETFyJF4BXEbVTTxROk8p6SQiIiZg83PgR8C7S2cZdOkkImIkSewF\n",
       "fB94ic3jpfOUkE4iIqINm1uBq4H3lM4yyNJJRMTIktgD+HequYlHS+fpt3QSERGTsLkDuAJ4b+ks\n",
       "gyqdRESMNIndqC5zuqvNutJ5+imdRETERtjcDVwC/F3pLIMonUREjDyJlwDXUp3T6Tel8/RLOomI\n",
       "iCmwuRf4LvD+0lkGTTqJiAhAYhfgBqrrTawtHKcv0klEREyRzUrgW8AHCkcZKOkkIiJqEjsBNwJ7\n",
       "2DxYOs9MSycREdEBm18CFwDHlc4yKNJJRES0kNgRuAVYaPNA6TwzKZ1ERESHbFYD5wEfLJ1lEKST\n",
       "iIgYR2I74HZgL5v7S+eZKekkIiKmoT7M9CVgWekspaWTiIiYgMS2VN3EK2xWlc4zE9JJRERMk80a\n",
       "4GzghNJZSkonERHRhsSLgbuAfWz+s3SeXksnERHRBZuHgLOAE0tnKSWdRETEJCS2Ae4G9rP5j9J5\n",
       "eimdREREl2weBv4F+FDpLCWkk4iI2AiJOcA9wH+1WVE6T6/0pZOQ9PeSnpW0TcvYMkkrJN0p6eCW\n",
       "8X0l3VKvO6NlfFNJ36jHr5G0c8u6oyTdXd/e1W3eiIhO2TwCnAH8Y+ks/dZVkZA0DzgINsz6S1oI\n",
       "HAEsBBYDZ0oaq1SfA5baXgAskLS4Hl8KrK3HTwdOrR9rG+AkYL/6drKkOd1kjoiYpjOAxRIvLR2k\n",
       "n7rtJP4X8A/jxg4DLrD9tO2VVC3a/pK2B7a0fV293XnA4fXyocC59fKFwAH18puBK2w/YvsR4Eqq\n",
       "whMR0Vc2j1J9iD2pdJZ+mnaRkHQYsMr2zeNW7QDP+XXiKmDHCcZX1+PUf+8DsL0eWCfpRZM8VkRE\n",
       "CZ8FDpDYs3SQfpk92UpJVwLbTbDqRKpzmhzcunkPc02LpFNa7i63vbxQlIgYQjaPSfxP4GTg7aXz\n",
       "dErSImBRJ/tMWiRsH9TmifYC5gM/r6cb5gI3SNqfqkOY17L5XKoOYHW9PH6cet1OwP2SZgNb214r\n",
       "afW4FzQP+MEkeU+Z7PVERPTAmcA9Ei+3GX8kZaDVH5yXj92XdPLG9pnW4Sbbt9re1vZ82/Op3uz3\n",
       "sb0GuBhYImkTSfOBBcB1th8AHpW0fz2R/U7govohLwaOqpffBny/Xr4COFjSHEl/RDVJfvl0MkdE\n",
       "9ILNE8BpwCmFo/TFpJ1EB/7wYwvbt0v6JtXZE9cDR3vDjzGOBr4MbAZcavuyevwc4HxJK4C1wJL6\n",
       "sR6W9BHgp/V2H64nsCMiSjoLOE5ib5sbS4eZSfkxXUTENEgcQ/UNzPdRfSB+pr5NumwzMG+6U3nf\n",
       "TJGIiJgGiRcAl1LNlc6iOjIzayPLs6iOvEy5qMzssv5+Y++bvTrcFBExUmyeBN7UyT4SopoLnmpR\n",
       "menljWdOJxERMZpyFtiIiOhKikRERLSVIhEREW2lSERERFspEhER0VaKREREtJUiERERbaVIRERE\n",
       "WykSERHRVopERES0lSIRERFtpUgMgPqSgo2V/GUlf1lNz78xKRKDYVHpAF1aVDpAlxaVDtClRaUD\n",
       "dGlR6QBdWlQ6wExKkYiIiLZSJCIioq2hup5E6QwREU0zMpcvjYiI3svhpoiIaCtFIiIi2mp8kZC0\n",
       "WNKdklZI+mDpPJ2Q9EVJayTdUjrLdEiaJ+lqSbdJulXSMaUzdULSCyRdK+kmSbdL+kTpTJ2SNEvS\n",
       "jZK+VzpLpyStlHRznf+60nk6JWmOpG9LuqP+38+rSmeaKkm71//uY7d17f7/2+g5CUmzgLuAA4HV\n",
       "wE+BI23fUTTYFEl6HfA4cJ7tl5XO0ylJ2wHb2b5J0guBG4DDm/LvDyBpc9u/lTQb+BHwAds/Kp1r\n",
       "qiS9H9gX2NL2oaXzdELSL4B9bT9cOst0SDoX+HfbX6z/97OF7XWlc3VK0vOo3j/3s33f+PVN7yT2\n",
       "A+6xvdL208DXgcMKZ5oy2/8H+E3pHNNl+wHbN9XLjwN3ADuUTdUZ27+tFzcBZgGNecOSNBc4BDgb\n",
       "mPQbKgOskbklbQ28zvYXAWyvb2KBqB0I3DtRgYDmF4kdgdYXtqoeiz6TtAuwN3Bt2SSdkfQ8STcB\n",
       "a4Crbd9eOlMHTgeOA54tHWSaDFwl6XpJ/710mA7NB34t6UuSfibpXyVtXjrUNC0BvtZuZdOLRHOP\n",
       "lQ2R+lDTt4Fj646iMWw/a/uVwFzg9U05D4+ktwIP2r6Rhn4aB15je2/gLcDf1odfm2I2sA9wpu19\n",
       "gCeA48tG6pykTYA/Bb7VbpumF4nVwLyW+/OouonoE0nPBy4EvmL7u6XzTFd9qOAS4E9KZ5miVwOH\n",
       "1sf1LwDeJOm8wpk6YvtX9d9fA9+hOnzcFKuAVbZ/Wt//NlXRaJq3ADfU/w0m1PQicT2wQNIudUU8\n",
       "Ari4cKaRIUnAOcDttv+5dJ5OSXqxpDn18mbAQcCNZVNNje0TbM+zPZ/qcMEPbL+rdK6pkrS5pC3r\n",
       "5S2Ag4HGfMvP9gPAfZJ2q4cOBG4rGGm6jqT6kNHW7D4FmRG210t6D3A51aTjOQ37Zs0FwBuAF0m6\n",
       "DzjJ9pcKx+rEa4B3ADdLGntzXWb7soKZOrE9cG797Y7nAefb/n7hTNPVtEOv2wLfqT5nMBv4qu0r\n",
       "ykbq2HuBr9YfUO8F/rJwno7UxflAYNL5oEZ/BTYiImZW0w83RUTEDEqRiIiItlIkIiKirRSJiIho\n",
       "K0UiIiLaSpGIiIi2UiQiIqKtFImIiGjr/wEQlcP2LUx+VQAAAABJRU5ErkJggg==\n"
      ],
      "text/plain": [
       "<matplotlib.figure.Figure at 0x107665cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(8), w[1:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.) Plot a histogram of the residuals $(f(x) - y)$. What distribution does this resemble? NOTE: You may not use any library routine for linear regression or least squares solving. You may use any other linear algebra routines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": [
       "iVBORw0KGgoAAAANSUhEUgAAAaIAAAEACAYAAADx33KKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\n",
       "AAALEgAACxIB0t1+/AAAHxRJREFUeJzt3X+QVed93/H3xwhs2Wa8Jk5BwNrQYWkgtR3MRFA3tXHt\n",
       "kM02BmXcgmgrYcIEZRhsT9y6QNyp6GRqY2XqRFQD1kzwSLItMP5RBU/W/LDqnbQdi61sCRHBGtb2\n",
       "OtLarGw5sjyatFnsb/84z4XD1d177i6wz134vGbu7HOe8zzPec65d893z3Oee1YRgZmZWS6vyN0B\n",
       "MzO7sTkQmZlZVg5EZmaWlQORmZll5UBkZmZZORCZmVlWlYFIUrekAUnnJG0fo8yetP6kpGVVdSXN\n",
       "knRc0llJxyR1lNbtTOUHJK0u5S+XdCqtu7eUv0jS/5T0RNr+b0/kQJiZWR5NA5GkacB9QDewFNgg\n",
       "aUldmR5gUUR0AVuAfS3U3QEcj4jFwKNpGUlLgfWpfDewV5JSnX3A5rSdLkndKf8/Ap+NiGXA7cDe\n",
       "iRwIMzPLo+qK6FZgMCKGImIUOAisrSuzBngQICJOAB2S5lTUvVgn/bwtpdcCByJiNCKGgEFghaRb\n",
       "gJkR0Z/KPVSq80PgdSndAQy3tOdmZtYWbqpYPw94prT8LLCihTLzgLlN6s6OiJGUHgFmp/Rc4LEG\n",
       "bY2mdM1wygf4OPANSR8AXgO8u2KfzMysjVRdEbX6/B9VF0GN2oviGUNX8pyhTwJ/HhGdQA/w2Sto\n",
       "y8zMJlnVFdEw0Fla7uTyK5NGZeanMtMb5NeGzUYkzYmI82nY7bmKtoZTuj4f4O3A3QAR8ZikV0l6\n",
       "Q0T8uNxJSX6onpnZBEREKxcbV7SBMV8Ugeo7wAJgBvAksKSuTA/Qm9Irgceq6gL3ANtTegewO6WX\n",
       "pnIzgIWpvtK6ExRDewJ6ge6U/2VgY0ovAYbH2Jdotq85XsCu3H1wn66vfrlP7tM16Fdc6200vSKK\n",
       "iAuStgFHgWnA/og4I+mutP7+iOiV1CNpEHgJ2NSsbmp6N3BI0mZgCFiX6pyWdAg4DVwAtkY6EsBW\n",
       "4AHg5hT4jqT8jwD7Jf0hxRDfxmb7ZGZm7aVqaI6I+Crw1bq8++uWt7VaN+X/BHjPGHU+BnysQf43\n",
       "gTc3yP8OsGrMHTAzs7bmJyvk1Ze7Aw305e5AA325OzCGvtwdaKAvdwca6MvdgQb6cneggb7cHchF\n",
       "l0a+rm+SIq71DTczs+vMZJw7fUVkZmZZORCZmVlWDkRmZpaVA5GZmWXlQGRmZlk5EJmZWVYORGZm\n",
       "lpUDkZmZZeVAZGZmWTkQmZlZVg5EZmaWlQORmZll5UBkZmZZORCZmVlWDkRmZpaVA5GZmWVV+a/C\n",
       "zWz8JI37P076HzfajaryikhSt6QBSeckbR+jzJ60/qSkZVV1Jc2SdFzSWUnHJHWU1u1M5QckrS7l\n",
       "L5d0Kq27t5T/SUlPpNe3Jf3tRA6E2dUX43iZ3biaBiJJ04D7gG5gKbBB0pK6Mj3AoojoArYA+1qo\n",
       "uwM4HhGLgUfTMpKWAutT+W5gr6TaX4n7gM1pO12SugEi4sMRsSwilgH/DfjSRA+GmZlNvqoroluB\n",
       "wYgYiohR4CCwtq7MGuBBgIg4AXRImlNR92Kd9PO2lF4LHIiI0YgYAgaBFZJuAWZGRH8q91CpTtm/\n",
       "Bg5U7JOZmbWRqkA0D3imtPxsymulzNwmdWdHxEhKjwCzU3puKteorXL+cH0/JL0JWAD8j4p9MjOz\n",
       "NlIViFodvG7lJqsatRcRV2uQ/HbgC6k9MzObIqpmzQ0DnaXlTi6/MmlUZn4qM71B/nBKj0iaExHn\n",
       "07DbcxVtDad0o7Zq1gNbm+2MpF2lxb6I6GtW3szsRiNpFbBqUrfZ7AJC0k3At4F3Az8A+oENEXGm\n",
       "VKYH2BYRPZJWAn8WESub1ZV0D/B8RHxC0g6gIyJ2pMkKD1PcX5oHfI1iIkRIOgF8MLXzl8CeiDiS\n",
       "+vArwFcjYmGTfQlPj7XJUkzfHs/FuTx929rSZJw7m14RRcQFSduAo8A0YH8KJHel9fdHRK+kHkmD\n",
       "wEvApmZ1U9O7gUOSNgNDwLpU57SkQ8Bp4AKwtTTUthV4ALgZ6K0FoWQ9nqRgZjYlNb0iup74isgm\n",
       "k6+I7HoxGedOP+LHzMyyciAyM7OsHIjMzCwrByIzM8vKgcjMzLJyIDIzs6wciMzMLCsHIjMzy8qB\n",
       "yMzMsnIgMjOzrByIzMwsKwciMzPLqur/EZndsIoHl46PH1xqNn4ORGZNje8J2mY2fh6aMzOzrByI\n",
       "zMwsKwciMzPLyoHIzMyyciAyM7OsKgORpG5JA5LOSdo+Rpk9af1JScuq6kqaJem4pLOSjknqKK3b\n",
       "mcoPSFpdyl8u6VRad2/d9tdJelrSX0v63HgPgpmZ5dM0EEmaBtwHdANLgQ2SltSV6QEWRUQXsAXY\n",
       "10LdHcDxiFgMPJqWkbQUWJ/KdwN7JdXmxO4DNqftdEnqTnW6Uv23R8Q/Bj40wWNhZmYZVF0R3QoM\n",
       "RsRQRIwCB4G1dWXWAA8CRMQJoEPSnIq6F+ukn7el9FrgQESMRsQQMAiskHQLMDMi+lO5h0p1fh+4\n",
       "LyJ+mvrw45b33szMsqsKRPOAZ0rLz6a8VsrMbVJ3dkSMpPQIMDul56Zyjdoq5w+X2uoC/pGk/yXp\n",
       "G5J+q2KfzMysjVQ9WaHVr5W38pVyNWovImIij1IpmQ4sAt4JdAJ/JenNtSukyzog7Sot9kVE3xVs\n",
       "18zsuiNpFbBqMrdZFYiGKU7uNZ1cfmXSqMz8VGZ6g/zhlB6RNCcizqdht+cq2hpO6fp8KK66TkTE\n",
       "z4EhSWcpAtM363cmInaNuadmZkb6A72vtizp7mu9zaqhuccpJgYskDSDYiLB4boyh4E7ASStBF5I\n",
       "w27N6h4GNqb0RuCRUv7tkmZIWkgx7NYfEeeBFyWtSJMX7gD+ItV5hBS9Jb0BWAx8dxzHwMzMMmp6\n",
       "RRQRFyRtA44C04D9EXFG0l1p/f0R0SupR9Ig8BKwqVnd1PRu4JCkzcAQsC7VOS3pEHAauABsjYja\n",
       "sN1W4AHgZqA3Io6kOkclrZb0NPBz4N9HxN9e8ZExM7NJoUvn+eubpPAj+m08inuX43v6du0zdiV1\n",
       "zdrJZJw7/WQFMzPLyoHIzMyyciAyM7OsHIjMzCwrByIzM8vKgcjMzLJyIDIzs6wciMzMLCsHIjMz\n",
       "y8qByMzMsnIgMjOzrByIzMwsKwciMzPLyoHIzMyyciAyM7OsHIjMzCwrByIzM8vKgcjMzLJyIDIz\n",
       "s6wqA5GkbkkDks5J2j5GmT1p/UlJy6rqSpol6biks5KOSeoorduZyg9IWl3KXy7pVFp3byn//ZJ+\n",
       "JOmJ9Pq9iRwIMzPLo2kgkjQNuA/oBpYCGyQtqSvTAyyKiC5gC7Cvhbo7gOMRsRh4NC0jaSmwPpXv\n",
       "BvZKUqqzD9icttMlqTvlB3AgIpal16cndijMzCyHqiuiW4HBiBiKiFHgILC2rswa4EGAiDgBdEia\n",
       "U1H3Yp3087aUXksRVEYjYggYBFZIugWYGRH9qdxDpTpKLzMzm4KqAtE84JnS8rMpr5Uyc5vUnR0R\n",
       "Iyk9AsxO6bmpXKO2yvnDpbYCeJ+kpyR9QdL8in0yM7M2clPF+mixnVauSNSovYgISa1up5GvAA9H\n",
       "xKikLRRXWO9u2AFpV2mxLyL6rmC7ZmbXHUmrgFWTuc2qQDQMdJaWO7n8yqRRmfmpzPQG+cMpPSJp\n",
       "TkScT8Nuz1W0NZzSL2srIn5Syt8P3DPWzkTErrHWmZkZpD/Q+2rLku6+1tusGpp7nGJiwAJJMygm\n",
       "EhyuK3MYuBNA0krghTTs1qzuYWBjSm8EHinl3y5phqSFQBfQHxHngRclrUiTF+6o1Un3o2rWAKdb\n",
       "330zM8ut6RVRRFyQtA04CkwD9kfEGUl3pfX3R0SvpB5Jg8BLwKZmdVPTu4FDkjYDQ8C6VOe0pEMU\n",
       "weQCsDUiasN2W4EHgJuB3og4kvI/KGlNKv888P4rOSBmZja5dOk8f32TFBHh2XXWsuLe5Xh+P0Tt\n",
       "M3Yldc3ayWScO/1kBTMzy8qByMzMsnIgMjOzrByIzMwsKwciMzPLyoHIzMyyciAyM7OsHIjMzCwr\n",
       "ByIzM8vKgcjMzLJyIDIzs6wciMzMLCsHIjMzy8qByMzMsnIgMjOzrByIzMwsKwciMzPLyoHIzMyy\n",
       "ciAyM7OsKgORpG5JA5LOSdo+Rpk9af1JScuq6kqaJem4pLOSjknqKK3bmcoPSFpdyl8u6VRad2+D\n",
       "PrxP0i8kvW08B8DMzPJqGogkTQPuA7qBpcAGSUvqyvQAiyKiC9gC7Guh7g7geEQsBh5Ny0haCqxP\n",
       "5buBvZKU6uwDNqftdEnqLvVhJvAh4LGJHAQzM8un6oroVmAwIoYiYhQ4CKytK7MGeBAgIk4AHZLm\n",
       "VNS9WCf9vC2l1wIHImI0IoaAQWCFpFuAmRHRn8o9VKoD8MfAbuD/AcLMzKaMqkA0D3imtPxsymul\n",
       "zNwmdWdHxEhKjwCzU3puKteorXL+cK2tNBQ3LyJ607qo2CczM2sjN1Wsb/Wk3spViBq1FxEhaULB\n",
       "Iw3bfRLY2EpfJO0qLfZFRN9Etmtmdr2StApYNZnbrApEw0BnabmTy69MGpWZn8pMb5A/nNIjkuZE\n",
       "xPk07PZcRVvDKV2fPxP4VaAv3UqaAxyW9N6I+Fb9zkTErmY7a2Z2o0t/oPfVliXdfa23WTU09zjF\n",
       "xIAFkmZQTCQ4XFfmMHAngKSVwAtp2K1Z3cNcuorZCDxSyr9d0gxJC4EuoD8izgMvSlqRroLuAP4i\n",
       "Il6MiF+OiIURsZBiskLDIGRmZu2p6RVRRFyQtA04CkwD9kfEGUl3pfX3R0SvpB5Jg8BLwKZmdVPT\n",
       "u4FDkjYDQ8C6VOe0pEPAaeACsDUiasN2W4EHgJuB3og4clWOgJmZZaVL5/nrm6SICM+os5YV9y7H\n",
       "8/shap+xK6lr1k4m49zpJyuYmVlWDkRmZpaVA5GZmWXlQGRmZlk5EJmZWVYORGZmlpUDkZmZZeVA\n",
       "ZGZmWTkQmZlZVg5EZmaWlQORmZllVfVvIMxskk3k/3P5OXU2lTkQmbWl8T0w1Wwq89CcmZll5UBk\n",
       "ZmZZORCZmVlWDkRmZpaVA5GZmWXlQGRmZllVBiJJ3ZIGJJ2TtH2MMnvS+pOSllXVlTRL0nFJZyUd\n",
       "k9RRWrczlR+QtLqUv1zSqbTu3lL+H0h6StITkr4h6a0TORBmZpZH00AkaRpwH9ANLAU2SFpSV6YH\n",
       "WBQRXcAWYF8LdXcAxyNiMfBoWkbSUmB9Kt8N7JVU+5LEPmBz2k6XpO6U/7mIeEtELAM+BvzXCR0J\n",
       "MzPLouqK6FZgMCKGImIUOAisrSuzBngQICJOAB2S5lTUvVgn/bwtpdcCByJiNCKGgEFghaRbgJkR\n",
       "0Z/KPVSrExE/K/XltcCPW9pzMzNrC1VPVpgHPFNafhZY0UKZecDcJnVnR8RISo8As1N6LvBYg7ZG\n",
       "U7pmOOUDIGkr8GHgNcDbK/bJbiB+XI5Z+6sKRK3+Erfyi6tG7UVETORkUdfGXophvA3Ap4F3NeyA\n",
       "tKu02BcRfVeyXZsq/Lgcs1ZJWgWsmsxtVgWiYaCztNzJ5VcmjcrMT2WmN8gfTukRSXMi4nwadnuu\n",
       "oq3hlG7UVtnngU+NtTMRsWusdWZmBukP9L7asqS7r/U2q+4RPU4xMWCBpBkUEwkO15U5DNwJIGkl\n",
       "8EIadmtW9zCwMaU3Ao+U8m+XNEPSQqAL6I+I88CLklakyQt31OpIWlTqy78Anmp9983MLLemV0QR\n",
       "cUHSNuAoMA3YHxFnJN2V1t8fEb2SeiQNAi8Bm5rVTU3vBg5J2gwMAetSndOSDgGngQvA1oiojats\n",
       "BR4AbgZ6I+JIyt8m6T0U95F+VNu+mZlNDbp0nr++SQrfhL7xFPcfx3ePqPY5mYp1za62yTh3+skK\n",
       "ZmaWlQORmZll5UBkZmZZORCZmVlWDkRmZpaVA5GZmWXlQGRmZlk5EJmZWVYORGZmlpUDkZmZZeVA\n",
       "ZGZmWTkQmZlZVg5EZmaWlQORmZll5UBkZmZZORCZmVlWDkRmZpaVA5GZmWXlQGRmZlm1FIgkdUsa\n",
       "kHRO0vYxyuxJ609KWlZVV9IsScclnZV0TFJHad3OVH5A0upS/nJJp9K6e0v5H5b0dNr21yS9cbwH\n",
       "wtqXpBjvK3efzax1lYFI0jTgPqAbWApskLSkrkwPsCgiuoAtwL4W6u4AjkfEYuDRtIykpcD6VL4b\n",
       "2CtJqc4+YHPaTpek7pT/LWB5RLwV+CJwz3gPhLW7GMfLzKaSVq6IbgUGI2IoIkaBg8DaujJrgAcB\n",
       "IuIE0CFpTkXdi3XSz9tSei1wICJGI2IIGARWSLoFmBkR/ancQ7U6EdEXEf835Z8A5re092Zmll0r\n",
       "gWge8Exp+dmU10qZuU3qzo6IkZQeAWan9NxUrlFb5fzhBv0A2Az0jr07ZmbWTm5qoUyrYx2qLoIa\n",
       "tRcRV2VcX9K/Bd4G/OEY63eVFvsiou9Kt2lmdj2RtApYNZnbbCUQDQOdpeVOLr8yaVRmfiozvUH+\n",
       "cEqPSJoTEefTsNtzFW0Nc/mQW7ktJL0H+CPgHWkY8GUiYlfjXTQzMyhudQB9tWVJd1/rbbYyNPc4\n",
       "xcSABZJmUEwkOFxX5jBwJ4CklcALaditWd3DwMaU3gg8Usq/XdIMSQuBLqA/Is4DL0pakSYv3FGr\n",
       "k2bpfQp4b0T8eHyHwMzMcqq8IoqIC5K2AUeBacD+iDgj6a60/v6I6JXUI2kQeAnY1Kxuano3cEjS\n",
       "ZmAIWJfqnJZ0CDgNXAC2RkRt2G4r8ABwM9AbEUdS/j3Aa4Avpgl234+I2uQHMzNrY7p0jr++SYqI\n",
       "aOU+lrWZ4v7heD6novZe32h1za62yTh3+skKZmaWlQORmZll5UBkZmZZORCZmVlWDkRmZpaVA5GZ\n",
       "mWXlQGRmZlk5EJmZWVYORGZmlpUDkZmZZeVAZGZmWTkQmZlZVg5EZmaWlQORmZll5UBkZmZZORCZ\n",
       "mVlWDkRmZpaVA5GZmWXVUiCS1C1pQNI5SdvHKLMnrT8paVlVXUmzJB2XdFbSMUkdpXU7U/kBSatL\n",
       "+cslnUrr7i3lv0PStySNSnrfeA+CmZnlUxmIJE0D7gO6gaXABklL6sr0AIsiogvYAuxroe4O4HhE\n",
       "LAYeTctIWgqsT+W7gb2Sav8vfR+wOW2nS1J3yv8+sBF4eNxHwMzMsmrliuhWYDAihiJiFDgIrK0r\n",
       "swZ4ECAiTgAdkuZU1L1YJ/28LaXXAgciYjQihoBBYIWkW4CZEdGfyj1UqxMR34+IU8AvWt91MzNr\n",
       "B60EonnAM6XlZ1NeK2XmNqk7OyJGUnoEmJ3Sc1O5Rm2V84cb9MPMzKaYVgJRtNiWqougRu1FRIxj\n",
       "O2Zmdh25qYUyw0BnabmTy69MGpWZn8pMb5A/nNIjkuZExPk07PZcRVvDKd2orbIxA5qkXaXFvojo\n",
       "G6usmdmNSNIqYNVkbrOVQPQ4xcSABcAPKCYSbKgrcxjYBhyUtBJ4ISJGJD3fpO5higkGn0g/Hynl\n",
       "PyzpkxRDb11Af0SEpBclrQD6gTuAPXX9EE2uzCJiVwv7a2Z2w0p/oPfVliXdfa23WRmIIuKCpG3A\n",
       "UWAasD8izki6K62/PyJ6JfVIGgReAjY1q5ua3g0ckrQZGALWpTqnJR0CTgMXgK1p6A5gK/AAcDPQ\n",
       "GxFHACT9OvBl4PXA70jaFRFvvpIDY2Zmk0OXzvHXN0kREa3cx7I2IynGdwtR1N7rG62u2dU2GedO\n",
       "P1nBzMyyciAyM7OsHIjMzCyrVmbNmV2x4r7H+Pi+h9mNwYHIJtH4bsDb+Dng21TkQGR23XHAt6nF\n",
       "94jMzCwrByIzM8vKgcjMzLJyIDIzs6wciMzMLCsHIjMzy8qByMzMsvL3iKxl/rKkmV0LDkQ2Tv6y\n",
       "pJldXR6aMzOzrByIzMwsKw/N3YB8r8fM2okD0Q3L93rMrD1UDs1J6pY0IOmcpO1jlNmT1p+UtKyq\n",
       "rqRZko5LOivpmKSO0rqdqfyApNWl/OWSTqV195byXynp8yn/MUlvmsiBMLvRSYrxvnL32a4PTQOR\n",
       "pGnAfUA3sBTYIGlJXZkeYFFEdAFbgH0t1N0BHI+IxcCjaRlJS4H1qXw3sFdS7c/xfcDmtJ0uSd0p\n",
       "fzPwfMr/U+ATEzkQOUhalbsPL9eXuwNTSF/uDlwDMY7XJVMtiLXj71479mmyVF0R3QoMRsRQRIwC\n",
       "B4G1dWXWAA8CRMQJoEPSnIq6F+ukn7el9FrgQESMRsQQMAiskHQLMDMi+lO5h0p1ym19CXh3S3ve\n",
       "HlZNtOK1+8Xvm2iXbkB9uTvQZsYKWHc3yMtuVe4ONLAqdwdyqbpHNA94prT8LLCihTLzgLlN6s6O\n",
       "iJGUHgFmp/Rc4LEGbY2mdM1wyr9s+xFxQdJPJc2KiJ9U7NsVk157DKa9pfUao6Pwd++IiO+ljLsl\n",
       "3T2ebV4+acD3ecxs6qsKRK2e6Vo5y6lRexGR/TJ94l7xD+G1r4cZLfb/b6YD0y/PczAxm6ipOAO0\n",
       "WZ/H+sM0d5+vtapANAx0lpY7ufzKpFGZ+anM9Ab5wyk9ImlORJxPw27PVbQ1nNL1+bU6bwR+IOkm\n",
       "4HVjXQ1dm4D3s/FW+Pal214w3uBy+T5cq7r/uUndduxvzrqXH6up0eccdZt9piZXbbvjHY3Iaer+\n",
       "sd6aqkD0OMXEgAXADygmEmyoK3MY2AYclLQSeCEiRiQ936TuYWAjxcSCjcAjpfyHJX2SYsitC+hP\n",
       "V00vSloB9AN3AHvq2noM+JcUkx9e5nr/i8LMbKpqGojSPZdtwFFgGrA/Is5Iuiutvz8ieiX1SBoE\n",
       "XgI2Naubmt4NHJK0GRgC1qU6pyUdAk4DF4CtEVH7S2Ar8ABwM9AbEUdS/n7gM5LOAc8Dt1/RETEz\n",
       "s0mlS+d5MzOzDCJiSr2Afwf8AphVytsJnAMGgNWl/OXAqbTu3lL+K4HPp/zHgDeV1m0EzqbXnaX8\n",
       "hcCJVOcgMD3l/zFwEniSYliwM3e/gD8BzqR+fZnivlnuPv0r4Gng58Db6t7TbO/fBD+D3amv54Dt\n",
       "V+lz/WmKGaSnSnmzgONpX44BHZN8zDqBr6f37a+BD+buF/Cq9D4+STFy8vHcfSqtmwY8AXylHfpE\n",
       "Mdr0VOpTfzv0aczP/9X4JZqsF8UvxhHge6RARPHl1ycpTnYLKL57VLvS6wduTeleoDultwJ7U3o9\n",
       "cLD0Jn0H6Eiv75BO4sAhYF1K7wP+IKVnlvr3AeDPc/cL+E3gFSlvN7C7Dfr0K8BiihPb20rHLOv7\n",
       "N4HP4LTUxwWpz08CS67CZ/ufAcu4PBDdA/yHlN4+ie9jR1o3B/i1lH4t8G1gSRv069Xp500UJ8Df\n",
       "yN2ntP7DwOeAw23y/l08T7bLZ2rMz/+V/gJN5gv4AvAWLg9EOyn9VUoRqFYCtwBnSvm3A58qlVlR\n",
       "+jD/KKU3APtKdT6V6gn4EZdO7iuBIw36t7P0xrZFv4DfBT7bLn3i5YEoe5/G+Rn8J3X7swPYcZU+\n",
       "3wu4PBANUHznDoqgMDBZx2yM/j0CvKdd+gW8Gvg/wK/m7hPFTN6vAe/i0hVR7j59D/ilumPWFu9d\n",
       "/WvK/BsISWuBZyPiqbpVc7l8Snn5C7UtfQkW+KmkX2rS1iyK2YC/aNAWkv6LpL8B3g98vF36lfwe\n",
       "xV8x7dSnsnbsUzNjfYH7Wmj2xe9rfcwuk2a/LqMYFsvaL0mvkPRk2vbXI+Lp3H2ieLzYRyhuG9Tk\n",
       "7lMAX5P0uKTfb5M+NdRWT9+WdJwiStf7KEXEXl0uPimdKt7MLwFvlHQq5U0HFkh6b0R8JSI+CnxU\n",
       "0g7gz0gzB6+xLRTP2avv16tqBSR9FPj7iHh4EvrTUp8yiDZvr7WNRr4vfkt6LcXvwIci4mfl78Hl\n",
       "6Ff6g+LXJL0OOCrpXXXrJ7VPkn4HeC4inhjreXGZ3r9/GhE/lPTLwHFJA23Qp4ba6oooIn4zIt5c\n",
       "/wK+S3Gz+aSk71FcBn9T0myu7EuwlL4E+3yDtjpT3ruAF4G3pv5sAvoi4it1u/Aw8OulbVzLfv0n\n",
       "iiGJ+n4NpPrvB3qAf1Oql7VPY5iM9+8nFM9AfEWprWEmppUveV8tI+m5jVzFL363cswu7o+k6RRB\n",
       "6DMRUfu+X/Z+AUTET4G/pLiZnrNPbwfWpHPTAeCfS/pM7uMUET9MP38E/HeK53+2xXv3Ms3G7dr1\n",
       "RePJCjMogtV3uHST7QTF8+3Ey2+y7SuNeZZvsn2X4gbb62vptO4QsL405lmbrNBV6tcHKH5hs/aL\n",
       "YlbX08Ab6o5b1mOVlr8OLG+nPo3zs3dT6uOC1OerMlkhtb2Al09W2J7SO3j5jeVrfcxE8YDhP63r\n",
       "Z7Z+AW8o9e9m4K8oHnSc9ViVjs07uXSPKOdxejVpIhXwGuB/U4wotcVxetln/2r8Ak32K+1Yefr2\n",
       "H1HM8hgAfquUX5t2OAjsKeW/kuLEVJt2uKC0blPKPwdsLOWXp/9+nkvTt7+YtvEkxV+O/yB3v1L6\n",
       "+xTTNp8gzWzJ3KffpRhP/jvgPPDV3H26gs/fb1PMIBsEdl6lz/QBiieQ/H06TpsofqG/RuOptpNx\n",
       "zH6D4p7Hk6XPUnfOfgFvBr6V+vQU8JGUn/VYlda/k0uz5nIep4XpGD1JMfV+Z+4+NXv5C61mZpZV\n",
       "W90jMjOzG48DkZmZZeVAZGZmWTkQmZlZVg5EZmaWlQORmZll5UBkZmZZORCZmVlW/x+vT109b5lH\n",
       "QwAAAABJRU5ErkJggg==\n"
      ],
      "text/plain": [
       "<matplotlib.figure.Figure at 0x107695358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "residuals = h_Ytest_hat - h_Ytest\n",
    "bins = np.linspace(np.min(residuals), np.max(residuals), num=20)\n",
    "plt.hist(residuals, bins, normed=1, histtype='bar', rwidth=0.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Problem 2: Logistic Regression\n",
    "\n",
    "Let $\\{(x^{(i)}, y^{(i)})^{n}\\}_{i=1}$ be a training set, where $x^{i} \\in \\mathbb{R}^{d}$ and $y^{(i)} \\in \\{-1, 1\\}$. Recall that the loss function for logistic regression is the cross-entropy. Therefore our risk is:\n",
    "\n",
    "$$R[\\mathbf{w}] = \\sum^{n}_{i=1} \\log(1 + e^{-z^{(i)}})$$\n",
    "\n",
    "where $f(x) = \\mathbf{w}^{T}\\mathbf{x}, z^{(i)} = y^{(i)}f(x^{(i)}).$\n",
    "\n",
    "In this problem you will minimize the cross-entropy risk (also known as the negative log likelihood of $\\mathbf{w}$) on a small training set. We have four data points in $\\mathbb{R}^{2}$, two of class 1, and two of class -1. Here is the data (and you may want to draw this on paper to see what it looks like):\n",
    "\n",
    "$$\n",
    "X =\n",
    "\\begin{bmatrix}\n",
    "    0 & 3 \\\\\n",
    "    1 & 3 \\\\\n",
    "    0 & 1 \\\\\n",
    "    1 & 1\n",
    "\\end{bmatrix}\n",
    ", y = \n",
    "\\begin{bmatrix}\n",
    "    1 \\\\\n",
    "    1 \\\\\n",
    "    -1 \\\\\n",
    "    -1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Here, $X$ is the training data matrix; each row $\\mathbf{x}^{(i)}$ of $X$ is a data point. Notice that the data cannot be separated by a boundary that goes through the origin. To account for this, you should append 1 to the $\\mathbf{x}^{(i)}$ vectors and fit a three-dimensional $\\mathbf{w}$ vector that includes an offset term.\n",
    "\n",
    "1.) Derive the gradient of the cross-entropy risk with respect to $\\mathbf{w}$. Show your work. Your answer should be in matrix-vector expression. Do NOT write your answer in terms of the individual components of the gradient. For notation, you may let the $diag(\\mathbf{v})$ denote the square matrix with components of vector $\\mathbf{v}$ on the diagonal. You may let $Q = diag(\\mathbf{y})X$, and you may assume $e^{M}$ is a matrix, where the $i$, $jth$ entry of $e^{M}$ is $e^{M_{ij}}$. Note: Writing these updates as a matrix operations instead of for-loops makes for cleaner code and allows you to take advantage of highly optimized linear algebra routines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.) In general, to verify that the function we minimize is convex we will want to show that the Hessian is positive semi-definite. For now, just derive the Hessian of the risk. Show your work; your answer should be a (somewhat complicated) matrix-vector expression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.) We will now perform gradient descent for a few iterations. Set the learning rate ($\\eta$) as 1. We are given that $  \\begin{equation}\n",
    "    \\mathbf{w}^{(0)}=\\begin{bmatrix}\n",
    "        -2 & 1 & 0\n",
    "    \\end{bmatrix}^{T}\n",
    "\\end{equation}$. $\\mathbf{w}^{(0)}$ is the value of $\\mathbf{w}$ at the $0^{th}$ iteration.\n",
    "\n",
    "3a.) State the value of $\\mathbf{\\mu}^{(0)} = P(Y = 1 | X = x)$. This should be an n-dimensional vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3b.) State the value of $\\mathbf{w}^{(1)}$ (the value of $\\mathbf{w}$ after one iteration)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3c.) State the value of $\\mathbf{\\mu}^{(1)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3d.) After performing a second iteration, state the value of $\\mathbf{w}^{(2)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Problem 3: Spam classification using Logistic Regression\n",
    "\n",
    "The spam dataset given to you as part of the homework in `spam.mat` consists of 4601 email messages, from which 57 features have been extracted as follows:\n",
    "\n",
    "* 48 features giving the proportion (0 to 1) of words in a given message which match a given word on the list. The list contains words such as business, free, george, etc. (The data was collected by George Forman, so his name occurs quite a lot!)\n",
    "* 6 features giving the proportion (0 - 1) of characters in the email that match a given character on the list. The characters are `;( [ ! $ #`.\n",
    "* Feature 55: The average length of an uninterrupted sequence of capital letters\n",
    "* Feature 56: The length of the longest uninterrupted sequence of capital letters\n",
    "* Feature 57: The sum of the lengths of uninterrupted sequences of capital letters\n",
    "\n",
    "The dataset consists of a training set size 3450 and a test set of size 1151. One can imagine performing several kinds of preprocessing to this data matrix. Try each of the following separately:\n",
    "\n",
    "i. Standardize each column so they each have mean 0 and unit variance.\n",
    "\n",
    "ii. Transform the features using $x^{(i)}_{j} \\leftarrow \\log(x^{(i)}_{j} + 1)$.\n",
    "\n",
    "iii. Binarize the features using $x^{(i)}_{j} \\leftarrow \\mathbb{i}(x^{(i)}_{j} > 0)$. $\\mathbb{i}$ denotes an indicator variable.\n",
    "\n",
    "Note: You will need to tune the step size carefully to avoid numerical issues and to avoid a diverging training risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spam = scipy.io.loadmat('data/spam.mat')\n",
    "s_Xtrain = np.matrix(spam['Xtrain'])\n",
    "s_Ytrain = np.matrix(spam['Ytrain'])\n",
    "s_Xtest  = np.matrix(spam['Xtest'])\n",
    "\n",
    "# preprocess\n",
    "s_Xtrain_std = add_ones((s_Xtrain - s_Xtrain.mean(axis=0))/s_Xtrain.std(axis=0))\n",
    "s_Xtrain_log = add_ones(np.log(s_Xtrain + 0.1))\n",
    "s_Xtrain_bin = add_ones((s_Xtrain > 0).astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.) Implement logistic regression to classify the spam data. Use batch gradient descent. Plot the training risk (the cross-entropy risk of the training set) vs. the number of iterations. You should have one plot for each preprocessing method. Note: One batch gradient descent iteration amounts to scanning through the whole training data and computing the full gradient. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.) Derive stochastic gradient descent equations for logistic regression and show your steps. Plot the training risk vs. number of iterations. You should have one plot for each preprocessing method. How are the plots different from (1)? Note: One stochastic gradient descent iteration amounts to computing the gradient using one data point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.) Instead of a constant learning rate $(\\eta)$, repeat (2) where the learning rate decreases as $\\eta \\propto 1/t$ for the $t^{th}$ iteration. Plot the training risk vs number of iterations. Is this strategy better than having a constant $ \\eta$? You should have one plot for each preprocessing method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4a.) Now let's use kernel logistic regression with a polynomial kernel of degree 2. Our risk is still the same as in problem 2, but our classifier is now:\n",
    "\n",
    "$$f(\\mathbf{x}) = \\sum^{n}_{i = 1} \\alpha_{i} K(\\mathbf{x}^{(i)}, \\mathbf{x}) \\textrm{ where } K(\\mathbf{x}^{(i)}, \\mathbf{x}) = (\\mathbf{x}^{T} \\mathbf{x}^{(i)} + 1)^{2}$$\n",
    "\n",
    "instead of $f(\\mathbf{x}) = \\mathbf{w}^{T}\\mathbf{x}$ as it was originally. Show that the stochastic gradient descent update for data point $\\mathbf{x}^{(i)}$ is:\n",
    "\n",
    "$$\\alpha_{i} \\leftarrow \\alpha_{i} + \\eta S(-z^{(i)})y^{(i)}$$\n",
    "\n",
    "$$\\textrm{Where } z^{(i)} = y^{(i)} f(\\mathbf{x}^{(i)}), S(-z^{(i)}) = \\dfrac{1}{1 + exp(z^{(i)})}$$\n",
    "\n",
    "Note that this measure means $\\mathbf{\\alpha} \\in \\mathbb{R}^{n}$. Also derive $\\dfrac{\\partial L}{\\partial \\alpha_{i}}$ directly from the loss function. You should get a different answer than the dual algorithm shown above. Under what condition are the two updates you derived the same?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4b.) Finally, repeat (2), using the best preprocessing method you found, using kernel logistic ridge regression. Use whichever learning rate scheme you wish. $\\gamma = 10^{-5}$. You  may optionally adjust the value of $\\gamma$. Generate a plot of training risk vs. number of iterations, and another plot of validation risk vs. number of iterations (use a 2/3, 1/3\n",
    "split). Use the following update equations (you do not need to derive them):\n",
    "\n",
    "$$\\alpha_{i} \\leftarrow \\alpha_{i} - \\gamma \\alpha_{i} + \\eta S(-z^{(i)})y^{(i)}$$\n",
    "\n",
    "$$\\alpha_{h} \\leftarrow \\alpha_{h} - \\gamma \\alpha_{h} \\textrm{ for } h \\neq i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the same experiment with the linear kernel $K(\\mathbf{x}^{(i)}, \\mathbf{x}) = \\mathbf{x}^{T} \\mathbf{x}^{(i)} + 1$. Does the quadratic kernel overfit the data? For each kernel, should you decrease or increase $\\gamma$ to try to improve performance?\n",
    "\n",
    "NOTE: You are NOT supposed to use any kind of software package for logistic regression!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4: Real World Spam Classification\n",
    "\n",
    "Daniel recently interned as an anti-spam product manager for a large email service provider. His company uses a linear SVM to predict whether an incoming spam message is spam or ham. He notices that the number of spam messages received tends to spike massively upwards a couple minutes before and after midnight. Eager to obtain a return offer, he adds the time-stamp of the received message, stored as number of milliseconds since the previous midnight, to each feature vector for the SVM to train on, in hopes that the ML model will be able to identify the abnormal spike in spam volume at night. To his dismay, after A/B testing with his newly added feature, Daniel discovers that the linear SVM's success rate barely improves. He wants to try to use a kernel to improve his model, but unfortunately he is limited to a quadratic kernel.\n",
    "\n",
    "Why can't the linear SVM utilize the new feature well, and what can Daniel do to improve his results? This is an actual interview question Daniel received for a machine learning engineering position!\n",
    "\n",
    "Write a short explanation. This question is open ended and there can be many correct answers. It is not necessary, but feel free to do your own research as long as you cite any sources."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
